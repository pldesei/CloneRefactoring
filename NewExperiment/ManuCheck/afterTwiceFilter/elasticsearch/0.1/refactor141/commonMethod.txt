(startLine=166 endLine=233 srcPath=/root/NewExperiment/elasticsearchFilter/00905/src/main/java/org/elasticsearch/percolator/PercolatorService.java)
    private PercolateShardResponse preparePercolate(PercolateShardRequest request, PercolateAction action) {
        IndexService percolateIndexService = indicesService.indexServiceSafe(request.index());
        IndexShard indexShard = percolateIndexService.shardSafe(request.shardId());

        ShardPercolateService shardPercolateService = indexShard.shardPercolateService();
        shardPercolateService.prePercolate();
        long startTime = System.nanoTime();
        try {
            ConcurrentMap<Text, Query> percolateQueries = indexShard.percolateRegistry().percolateQueries();
            if (percolateQueries.isEmpty()) {
                return new PercolateShardResponse(request.index(), request.shardId());
            }

            final PercolateContext context = new PercolateContext();
            context.percolateQueries = percolateQueries;
            context.indexShard = indexShard;
            ParsedDocument parsedDocument = parsePercolate(percolateIndexService, request, context);
            if (request.docSource() != null && request.docSource().length() != 0) {
                parsedDocument = parseFetchedDoc(request.docSource(), percolateIndexService, request.documentType());
            } else if (parsedDocument == null) {
                throw new ElasticSearchParseException("No doc to percolate in the request");
            }

            if (context.size < 0) {
                context.size = 0;
            }

            // first, parse the source doc into a MemoryIndex
            final MemoryIndex memoryIndex = cache.get();
            try {
                // TODO: This means percolation does not support nested docs...
                // So look into: ByteBufferDirectory
                for (IndexableField field : parsedDocument.rootDoc().getFields()) {
                    if (!field.fieldType().indexed()) {
                        continue;
                    }
                    // no need to index the UID field
                    if (field.name().equals(UidFieldMapper.NAME)) {
                        continue;
                    }
                    TokenStream tokenStream;
                    try {
                        tokenStream = field.tokenStream(parsedDocument.analyzer());
                        if (tokenStream != null) {
                            memoryIndex.addField(field.name(), tokenStream, field.boost());
                        }
                    } catch (IOException e) {
                        throw new ElasticSearchException("Failed to create token stream", e);
                    }
                }

                context.docSearcher = memoryIndex.createSearcher();
                context.fieldDataService = percolateIndexService.fieldData();
                IndexCache indexCache = percolateIndexService.cache();
                try {
                    return action.doPercolateAction(context);
                } finally {
                    // explicitly clear the reader, since we can only register on callback on SegmentReader
                    indexCache.clear(context.docSearcher.getIndexReader());
                    context.fieldDataService.clear(context.docSearcher.getIndexReader());
                }
            } finally {
                memoryIndex.reset();
            }
        } finally {
            shardPercolateService.postPercolate(System.nanoTime() - startTime);
        }
    }

