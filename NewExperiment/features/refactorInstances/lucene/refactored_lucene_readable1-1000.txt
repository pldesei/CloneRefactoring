[Instance #
frags: 
(startLine=20 endLine=56 srcPath=/home/sonia/NewExperiment/luceneFilter/00032/src/test/org/apache/lucene/search/TestRangeQuery.java)
  public void testExclusive() throws Exception {
    Directory dir = new RAMDirectory();
    Query query = new RangeQuery(new Term("content", "A"), new Term("content", "C"), false);
    Hits hits = null;

    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
    addDoc(writer, "A");
    addDoc(writer, "B");
    addDoc(writer, "C");
    addDoc(writer, "D");
    writer.close();

    IndexSearcher searcher = new IndexSearcher(dir);
    hits = searcher.search(query);
    assertEquals(1, hits.length());
    searcher.close();

    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
    addDoc(writer, "A");
    addDoc(writer, "B");
    addDoc(writer, "D");
    writer.close();

    searcher = new IndexSearcher(dir);
    hits = searcher.search(query);
    assertEquals(1, hits.length());
    searcher.close();

    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
    addDoc(writer, "C");
    writer.close();

    searcher = new IndexSearcher(dir);
    hits = searcher.search(query);
    assertEquals(1, hits.length());
    searcher.close();
  }

(startLine=58 endLine=96 srcPath=/home/sonia/NewExperiment/luceneFilter/00032/src/test/org/apache/lucene/search/TestRangeQuery.java)
  public void testInclusive() throws Exception {
    Directory dir = new RAMDirectory();
    IndexWriter writer = null;
    Searcher searcher = null;
    Query query = new RangeQuery(new Term("content", "A"), new Term("content", "C"), true);
    Hits hits = null;

    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
    addDoc(writer, "A");
    addDoc(writer, "B");
    addDoc(writer, "C");
    addDoc(writer, "D");
    writer.close();

    searcher = new IndexSearcher(dir);
    hits = searcher.search(query);
    assertEquals(3, hits.length());
    searcher.close();

    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
    addDoc(writer, "A");
    addDoc(writer, "B");
    addDoc(writer, "D");
    writer.close();

    searcher = new IndexSearcher(dir);
    hits = searcher.search(query);
    assertEquals(2, hits.length());
    searcher.close();

    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
    addDoc(writer, "C");
    writer.close();

    searcher = new IndexSearcher(dir);
    hits = searcher.search(query);
    assertEquals(3, hits.length());
    searcher.close();
  }

commonMethod: 
(startLine=72 endLine=78 srcPath=/home/sonia/NewExperiment/luceneFilter/00033/src/test/org/apache/lucene/search/TestRangeQuery.java)
  private void initializeIndex(String[] values) throws IOException {
    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
    for (int i = 0; i < values.length; i++) {
      insertDoc(writer, values[i]);
    }
    writer.close();
  }


, Instance #
frags: 
(startLine=150 endLine=166 srcPath=/home/sonia/NewExperiment/luceneFilter/00087/sandbox/projects/appex/src/java/search/DocumentHandler.java)
        {
            // add the custom fields
            for (Iterator it = customFields.keySet().iterator(); it.hasNext();)
            {
                String field = (String) it.next();
                String value = (String) metadata.get(field);
                String type = (String) customFields.get(field);
                addFieldToDoc(type, field, value);
            }
            // Add OBJECT_CLASS_FIELD and OBJECT_IDENTIFIER
            // to populate the result templates with the proper
            // objects
            doc.add(Field.UnIndexed(DataSource.OBJECT_CLASS,
                                    (String) metadata.get(DataSource.OBJECT_CLASS)));
            doc.add(Field.Text(DataSource.OBJECT_IDENTIFIER,
                               (String) metadata.get(DataSource.OBJECT_IDENTIFIER)));
        }

(startLine=168 endLine=183 srcPath=/home/sonia/NewExperiment/luceneFilter/00087/sandbox/projects/appex/src/java/search/DocumentHandler.java)
        {
            for (Iterator it = customFields.keySet().iterator(); it.hasNext();)
            {
                String field = (String) it.next();
                String value = parentDoc.get(field);
                String type = (String) customFields.get(field);
                addFieldToDoc(type, field, value);
            }
            // Add OBJECT_CLASS_FIELD and OBJECT_IDENTIFIER
            // to populate the result templates with the proper
            // objects
            doc.add(Field.UnIndexed(DataSource.OBJECT_CLASS,
                                    parentDoc.get(DataSource.OBJECT_CLASS)));
            doc.add(Field.Text(DataSource.OBJECT_IDENTIFIER,
                               parentDoc.get(DataSource.OBJECT_IDENTIFIER)));
        }

commonMethod: 
(startLine=169 endLine=194 srcPath=/home/sonia/NewExperiment/luceneFilter/00088/sandbox/projects/appex/src/java/search/DocumentHandler.java)
    /**
     * Add the contents of a Map to a document.
     *
     * @param Map to add.
     */
    private void addMapToDoc(Map map)
    {
        for (Iterator it = map.keySet().iterator(); it.hasNext();)
        {
            String field = (String) it.next();
            Object value = map.get(field);
            if (value instanceof String)
            {
                String type = null;
                if (customFields != null)
                {
                    type = (String) customFields.get(field);
                }
                addFieldToDoc(type, field, (String) value);
            }
            else if (value instanceof Reader)
            {
                addFieldToDoc(field, (Reader) value);
            }
        }
    }


, Instance #
frags: 
(startLine=150 endLine=166 srcPath=/home/sonia/NewExperiment/luceneFilter/00087/sandbox/projects/appex/src/java/search/DocumentHandler.java)
        {
            // add the custom fields
            for (Iterator it = customFields.keySet().iterator(); it.hasNext();)
            {
                String field = (String) it.next();
                String value = (String) metadata.get(field);
                String type = (String) customFields.get(field);
                addFieldToDoc(type, field, value);
            }
            // Add OBJECT_CLASS_FIELD and OBJECT_IDENTIFIER
            // to populate the result templates with the proper
            // objects
            doc.add(Field.UnIndexed(DataSource.OBJECT_CLASS,
                                    (String) metadata.get(DataSource.OBJECT_CLASS)));
            doc.add(Field.Text(DataSource.OBJECT_IDENTIFIER,
                               (String) metadata.get(DataSource.OBJECT_IDENTIFIER)));
        }

(startLine=168 endLine=183 srcPath=/home/sonia/NewExperiment/luceneFilter/00087/sandbox/projects/appex/src/java/search/DocumentHandler.java)
        {
            for (Iterator it = customFields.keySet().iterator(); it.hasNext();)
            {
                String field = (String) it.next();
                String value = parentDoc.get(field);
                String type = (String) customFields.get(field);
                addFieldToDoc(type, field, value);
            }
            // Add OBJECT_CLASS_FIELD and OBJECT_IDENTIFIER
            // to populate the result templates with the proper
            // objects
            doc.add(Field.UnIndexed(DataSource.OBJECT_CLASS,
                                    parentDoc.get(DataSource.OBJECT_CLASS)));
            doc.add(Field.Text(DataSource.OBJECT_IDENTIFIER,
                               parentDoc.get(DataSource.OBJECT_IDENTIFIER)));
        }

commonMethod: 
(startLine=196 endLine=220 srcPath=/home/sonia/NewExperiment/luceneFilter/00088/sandbox/projects/appex/src/java/search/DocumentHandler.java)
    /**
     * Add nested datasources.
     *
     * @param Map which contains the nested datasources.
     */
    private void addNestedDataSource(Map map) throws Exception
    {
        Object o = map.get(DataSource.NESTED_DATASOURCE);
        if (o == null)
            return;
        if (o instanceof List)
        {
            List nestedDataSource = (List) o;
            for (int i = 0; i < nestedDataSource.size(); i++)
            {
                DataSource ds = (DataSource) nestedDataSource.get(i);
                addDataSource(ds);
            }
        }
        else if (o instanceof DataSource)
        {
            DataSource ds = (DataSource) o;
            addDataSource(ds);
        }
    }


, Instance #
frags: 
(startLine=150 endLine=166 srcPath=/home/sonia/NewExperiment/luceneFilter/00087/sandbox/projects/appex/src/java/search/DocumentHandler.java)
        {
            // add the custom fields
            for (Iterator it = customFields.keySet().iterator(); it.hasNext();)
            {
                String field = (String) it.next();
                String value = (String) metadata.get(field);
                String type = (String) customFields.get(field);
                addFieldToDoc(type, field, value);
            }
            // Add OBJECT_CLASS_FIELD and OBJECT_IDENTIFIER
            // to populate the result templates with the proper
            // objects
            doc.add(Field.UnIndexed(DataSource.OBJECT_CLASS,
                                    (String) metadata.get(DataSource.OBJECT_CLASS)));
            doc.add(Field.Text(DataSource.OBJECT_IDENTIFIER,
                               (String) metadata.get(DataSource.OBJECT_IDENTIFIER)));
        }

(startLine=168 endLine=183 srcPath=/home/sonia/NewExperiment/luceneFilter/00087/sandbox/projects/appex/src/java/search/DocumentHandler.java)
        {
            for (Iterator it = customFields.keySet().iterator(); it.hasNext();)
            {
                String field = (String) it.next();
                String value = parentDoc.get(field);
                String type = (String) customFields.get(field);
                addFieldToDoc(type, field, value);
            }
            // Add OBJECT_CLASS_FIELD and OBJECT_IDENTIFIER
            // to populate the result templates with the proper
            // objects
            doc.add(Field.UnIndexed(DataSource.OBJECT_CLASS,
                                    parentDoc.get(DataSource.OBJECT_CLASS)));
            doc.add(Field.Text(DataSource.OBJECT_IDENTIFIER,
                               parentDoc.get(DataSource.OBJECT_IDENTIFIER)));
        }

commonMethod: 
(startLine=222 endLine=253 srcPath=/home/sonia/NewExperiment/luceneFilter/00088/sandbox/projects/appex/src/java/search/DocumentHandler.java)
    /**
     * Datasources are basically a collection of data maps to be indexed.
     * addMapToDoc is invoked for each map.
     *
     * @param Datasource to add.
     */
    private void addDataSource(DataSource ds) throws Exception
    {
        Map[] data = ds.getData();
        for (int i = 0; i < data.length; i++)
        {
            Map map = data[i];
            if (map.containsKey(DataSource.OBJECT_IDENTIFIER))
            {
                /**
                 * Create a new document because child datasources may need
                 * to be retrieved independently of parent doc.
                 */
                DocumentHandler docHandler = new DocumentHandler(map, null, null);
                docHandler.process();
                documents.addAll(docHandler.getDocuments());
            }
            else
            {
                addMapToDoc(map);
                /**
                 * Add nested datasources of this datasource's data
                 */
                addNestedDataSource(map);
            }
        }
    }


, Instance #
frags: 
(startLine=438 endLine=445 srcPath=/home/sonia/NewExperiment/luceneFilter/00095/sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherTask.java)
        {
            // router is down or firewall prevents to connect
            hi.setReachable(false);
            taskState.setState(FT_EXCEPTION);
            System.out.println("[" + threadNr + "] " + e.getClass().getName() + ": " + e.getMessage());
            // e.printStackTrace();
            errorLog.log("error: " + e.getClass().getName() + ": " + e.getMessage());
        }

(startLine=447 endLine=454 srcPath=/home/sonia/NewExperiment/luceneFilter/00095/sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherTask.java)
        {
            // no server is listening at this port
            hi.setReachable(false);
            taskState.setState(FT_EXCEPTION);
            System.out.println("[" + threadNr + "] " + e.getClass().getName() + ": " + e.getMessage());
            // e.printStackTrace();
            errorLog.log("error: " + e.getClass().getName() + ": " + e.getMessage());
        }

(startLine=463 endLine=471 srcPath=/home/sonia/NewExperiment/luceneFilter/00095/sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherTask.java)
        {
            // IP Address not to be determined
            hi.setReachable(false);
            taskState.setState(FT_EXCEPTION);
            System.out.println("[" + threadNr + "] " + e.getClass().getName() + ": " + e.getMessage());
            // e.printStackTrace();
            errorLog.log("error: " + e.getClass().getName() + ": " + e.getMessage());

        }

(startLine=473 endLine=479 srcPath=/home/sonia/NewExperiment/luceneFilter/00095/sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherTask.java)
        {
            taskState.setState(FT_EXCEPTION);
            System.out.println("[" + threadNr + "] " + e.getClass().getName() + ": " + e.getMessage());
            // e.printStackTrace();
            errorLog.log("error: IOException: " + e.getClass().getName() + ": " + e.getMessage());

        }

commonMethod: 
(startLine=101 endLine=104 srcPath=/home/sonia/NewExperiment/luceneFilter/00096/sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/util/WebDocument.java)
    public void addField(String name, Object value)
    {
        fields.put(name, value);
    }


, Instance #
frags: 
(startLine=254 endLine=280 srcPath=/home/sonia/NewExperiment/luceneFilter/00181/src/java/org/apache/lucene/search/PhraseQuery.java)
  public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (!field.equals(f)) {
      buffer.append(field);
      buffer.append(":");
    }

    buffer.append("\"");
    for (int i = 0; i < terms.size(); i++) {
      buffer.append(((Term)terms.elementAt(i)).text());
      if (i != terms.size()-1)
    buffer.append(" ");
    }
    buffer.append("\"");

    if (slop != 0) {
      buffer.append("~");
      buffer.append(slop);
    }

    if (getBoost() != 1.0f) {
      buffer.append("^");
      buffer.append(Float.toString(getBoost()));
    }

    return buffer.toString();
  }

(startLine=246 endLine=274 srcPath=/home/sonia/NewExperiment/luceneFilter/00181/src/java/org/apache/lucene/search/PhrasePrefixQuery.java)
  public final String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (!field.equals(f)) {
      buffer.append(field);
      buffer.append(":");
    }

    buffer.append("\"");
    Iterator i = termArrays.iterator();
    while (i.hasNext()) {
      Term[] terms = (Term[])i.next();
      buffer.append(terms[0].text() + (terms.length > 1 ? "*" : ""));
      if (i.hasNext())
        buffer.append(" ");
    }
    buffer.append("\"");

    if (slop != 0) {
      buffer.append("~");
      buffer.append(slop);
    }

    if (getBoost() != 1.0f) {
      buffer.append("^");
      buffer.append(Float.toString(getBoost()));
    }

    return buffer.toString();
  }

commonMethod: 
(startLine=20 endLine=24 srcPath=/home/sonia/NewExperiment/luceneFilter/00182/src/java/org/apache/lucene/util/ToStringUtils.java)
  public static String boost(float boost) {
    if (boost != 1.0f) {
      return "^" + Float.toString(boost);
    } else return "";
  }


, Instance #
frags: 
(startLine=149 endLine=161 srcPath=/home/sonia/NewExperiment/luceneFilter/00181/src/java/org/apache/lucene/search/TermQuery.java)
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    if (!term.field().equals(field)) {
      buffer.append(term.field());
      buffer.append(":");
    }
    buffer.append(term.text());
    if (getBoost() != 1.0f) {
      buffer.append("^");
      buffer.append(Float.toString(getBoost()));
    }
    return buffer.toString();
  }

(startLine=71 endLine=83 srcPath=/home/sonia/NewExperiment/luceneFilter/00181/src/java/org/apache/lucene/search/MultiTermQuery.java)
    public String toString(String field) {
        StringBuffer buffer = new StringBuffer();
        if (!term.field().equals(field)) {
            buffer.append(term.field());
            buffer.append(":");
        }
        buffer.append(term.text());
        if (getBoost() != 1.0f) {
            buffer.append("^");
            buffer.append(Float.toString(getBoost()));
        }
        return buffer.toString();
    }

(startLine=64 endLine=77 srcPath=/home/sonia/NewExperiment/luceneFilter/00181/src/java/org/apache/lucene/search/PrefixQuery.java)
  public String toString(String field) {
    StringBuffer buffer = new StringBuffer();
    if (!prefix.field().equals(field)) {
      buffer.append(prefix.field());
      buffer.append(":");
    }
    buffer.append(prefix.text());
    buffer.append('*');
    if (getBoost() != 1.0f) {
      buffer.append("^");
      buffer.append(Float.toString(getBoost()));
    }
    return buffer.toString();
  }

commonMethod: 
(startLine=20 endLine=24 srcPath=/home/sonia/NewExperiment/luceneFilter/00182/src/java/org/apache/lucene/util/ToStringUtils.java)
  public static String boost(float boost) {
    if (boost != 1.0f) {
      return "^" + Float.toString(boost);
    } else return "";
  }


, Instance #
frags: 
(startLine=802 endLine=816 srcPath=/home/sonia/NewExperiment/luceneFilter/00240/src/java/org/apache/lucene/index/IndexWriter.java)
    if (useCompoundFile) {
      final Vector filesToDelete = merger.createCompoundFile(mergedName + ".tmp");
      synchronized (directory) { // in- & inter-process sync
        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {
          public Object doBody() throws IOException {
            // make compound file visible for SegmentReaders
            directory.renameFile(mergedName + ".tmp", mergedName + ".cfs");
            return null;
          }
        }.run();
      }

      // delete now unused files of segment
      deleteFiles(filesToDelete);
    }

(startLine=980 endLine=994 srcPath=/home/sonia/NewExperiment/luceneFilter/00240/src/java/org/apache/lucene/index/IndexWriter.java)
    if (useCompoundFile) {
      final Vector filesToDelete = merger.createCompoundFile(mergedName + ".tmp");
      synchronized (directory) { // in- & inter-process sync
        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {
          public Object doBody() throws IOException {
            // make compound file visible for SegmentReaders
            directory.renameFile(mergedName + ".tmp", mergedName + ".cfs");
            return null;
          }
        }.run();
      }

      // delete now unused files of segment 
      deleteFiles(filesToDelete);   
    }

commonMethod: 
(startLine=168 endLine=179 srcPath=/home/sonia/NewExperiment/luceneFilter/00241/src/java/org/apache/lucene/index/IndexFileDeleter.java)
  public final void deleteFile(String file)
       throws IOException {
    try {
      directory.deleteFile(file);          // try to delete each file
    } catch (IOException e) {              // if delete fails
      if (directory.fileExists(file)) {
        if (infoStream != null)
          infoStream.println("IndexFileDeleter: unable to remove file \"" + file + "\": " + e.toString() + "; Will re-try later.");
        addDeletableFile(file);                  // add to deletable
      }
    }
  }


, Instance #
frags: 
(startLine=802 endLine=816 srcPath=/home/sonia/NewExperiment/luceneFilter/00240/src/java/org/apache/lucene/index/IndexWriter.java)
    if (useCompoundFile) {
      final Vector filesToDelete = merger.createCompoundFile(mergedName + ".tmp");
      synchronized (directory) { // in- & inter-process sync
        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {
          public Object doBody() throws IOException {
            // make compound file visible for SegmentReaders
            directory.renameFile(mergedName + ".tmp", mergedName + ".cfs");
            return null;
          }
        }.run();
      }

      // delete now unused files of segment
      deleteFiles(filesToDelete);
    }

(startLine=980 endLine=994 srcPath=/home/sonia/NewExperiment/luceneFilter/00240/src/java/org/apache/lucene/index/IndexWriter.java)
    if (useCompoundFile) {
      final Vector filesToDelete = merger.createCompoundFile(mergedName + ".tmp");
      synchronized (directory) { // in- & inter-process sync
        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {
          public Object doBody() throws IOException {
            // make compound file visible for SegmentReaders
            directory.renameFile(mergedName + ".tmp", mergedName + ".cfs");
            return null;
          }
        }.run();
      }

      // delete now unused files of segment 
      deleteFiles(filesToDelete);   
    }

commonMethod: 
(startLine=134 endLine=141 srcPath=/home/sonia/NewExperiment/luceneFilter/00241/src/java/org/apache/lucene/index/SegmentInfos.java)
  /**
   * Get the segment_N filename in use by this segment infos.
   */
  public String getCurrentSegmentFileName() {
    return IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                 "",
                                                 generation);
  }


, Instance #
frags: 
(startLine=802 endLine=816 srcPath=/home/sonia/NewExperiment/luceneFilter/00240/src/java/org/apache/lucene/index/IndexWriter.java)
    if (useCompoundFile) {
      final Vector filesToDelete = merger.createCompoundFile(mergedName + ".tmp");
      synchronized (directory) { // in- & inter-process sync
        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {
          public Object doBody() throws IOException {
            // make compound file visible for SegmentReaders
            directory.renameFile(mergedName + ".tmp", mergedName + ".cfs");
            return null;
          }
        }.run();
      }

      // delete now unused files of segment
      deleteFiles(filesToDelete);
    }

(startLine=980 endLine=994 srcPath=/home/sonia/NewExperiment/luceneFilter/00240/src/java/org/apache/lucene/index/IndexWriter.java)
    if (useCompoundFile) {
      final Vector filesToDelete = merger.createCompoundFile(mergedName + ".tmp");
      synchronized (directory) { // in- & inter-process sync
        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {
          public Object doBody() throws IOException {
            // make compound file visible for SegmentReaders
            directory.renameFile(mergedName + ".tmp", mergedName + ".cfs");
            return null;
          }
        }.run();
      }

      // delete now unused files of segment 
      deleteFiles(filesToDelete);   
    }

commonMethod: 
(startLine=268 endLine=280 srcPath=/home/sonia/NewExperiment/luceneFilter/00241/src/java/org/apache/lucene/index/SegmentInfo.java)
  /**
   * Mark whether this segment is stored as a compound file.
   *
   * @param isCompoundFile true if this is a compound file;
   * else, false
   */
  void setUseCompoundFile(boolean isCompoundFile) {
    if (isCompoundFile) {
      this.isCompoundFile = 1;
    } else {
      this.isCompoundFile = -1;
    }
  }


, Instance #
frags: 
(startLine=203 endLine=214 srcPath=/home/sonia/NewExperiment/luceneFilter/00293/src/java/org/apache/lucene/queryParser/MultiFieldQueryParser.java)
  {
    if (queries.length != fields.length)
      throw new IllegalArgumentException("queries.length != fields.length");
    BooleanQuery bQuery = new BooleanQuery();
    for (int i = 0; i < fields.length; i++)
    {
      QueryParser qp = new QueryParser(fields[i], analyzer);
      Query q = qp.parse(queries[i]);
      bQuery.add(q, BooleanClause.Occur.SHOULD);
    }
    return bQuery;
  }

(startLine=247 endLine=257 srcPath=/home/sonia/NewExperiment/luceneFilter/00293/src/java/org/apache/lucene/queryParser/MultiFieldQueryParser.java)
      BooleanClause.Occur[] flags, Analyzer analyzer) throws ParseException {
    if (fields.length != flags.length)
      throw new IllegalArgumentException("fields.length != flags.length");
    BooleanQuery bQuery = new BooleanQuery();
    for (int i = 0; i < fields.length; i++) {
      QueryParser qp = new QueryParser(fields[i], analyzer);
      Query q = qp.parse(query);
      bQuery.add(q, flags[i]);
    }
    return bQuery;
  }

commonMethod: 
(startLine=170 endLine=173 srcPath=/home/sonia/NewExperiment/luceneFilter/00294/src/java/org/apache/lucene/search/BooleanQuery.java)
  /** Returns the set of clauses in this query. */
  public BooleanClause[] getClauses() {
    return (BooleanClause[])clauses.toArray(new BooleanClause[clauses.size()]);
  }


, Instance #
frags: 
(startLine=200 endLine=238 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
  TermFreqVector[] get(int docNum) throws IOException {
    TermFreqVector[] result = null;
    if (tvx != null) {
      //We need to offset by
      tvx.seek(((docNum + docStoreOffset) * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      // No fields are vectorized for this document
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }
    } else {
      //System.out.println("No tvx file");
    }
    return result;
  }

(startLine=240 endLine=278 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
  public void get(int docNumber, TermVectorMapper mapper) throws IOException {
    // Check if no term vectors are available for this segment at all
    if (tvx != null) {
      //We need to offset by
      tvx.seek((docNumber * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      // No fields are vectorized for this document
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }
    } else {
      //System.out.println("No tvx file");
    }
  }

commonMethod: 
(startLine=128 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00347/src/java/org/apache/lucene/index/TermVectorsReader.java)
  final private void seekTvx(final int docNum) throws IOException {
    if (format < FORMAT_VERSION2)
      tvx.seek((docNum + docStoreOffset) * 8L + FORMAT_SIZE);
    else
      tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
  }


, Instance #
frags: 
(startLine=200 endLine=238 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
  TermFreqVector[] get(int docNum) throws IOException {
    TermFreqVector[] result = null;
    if (tvx != null) {
      //We need to offset by
      tvx.seek(((docNum + docStoreOffset) * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      // No fields are vectorized for this document
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }
    } else {
      //System.out.println("No tvx file");
    }
    return result;
  }

(startLine=240 endLine=278 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
  public void get(int docNumber, TermVectorMapper mapper) throws IOException {
    // Check if no term vectors are available for this segment at all
    if (tvx != null) {
      //We need to offset by
      tvx.seek((docNumber * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      // No fields are vectorized for this document
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }
    } else {
      //System.out.println("No tvx file");
    }
  }

commonMethod: 
(startLine=285 endLine=299 srcPath=/home/sonia/NewExperiment/luceneFilter/00347/src/java/org/apache/lucene/index/TermVectorsReader.java)
  final private String[] readFields(int fieldCount) throws IOException {
    int number = 0;
    String[] fields = new String[fieldCount];

    for (int i = 0; i < fieldCount; i++) {
      if (format >= FORMAT_VERSION)
        number = tvd.readVInt();
      else
        number += tvd.readVInt();

      fields[i] = fieldInfos.fieldName(number);
    }

    return fields;
  }


, Instance #
frags: 
(startLine=200 endLine=238 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
  TermFreqVector[] get(int docNum) throws IOException {
    TermFreqVector[] result = null;
    if (tvx != null) {
      //We need to offset by
      tvx.seek(((docNum + docStoreOffset) * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      // No fields are vectorized for this document
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }
    } else {
      //System.out.println("No tvx file");
    }
    return result;
  }

(startLine=240 endLine=278 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
  public void get(int docNumber, TermVectorMapper mapper) throws IOException {
    // Check if no term vectors are available for this segment at all
    if (tvx != null) {
      //We need to offset by
      tvx.seek((docNumber * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      // No fields are vectorized for this document
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }
    } else {
      //System.out.println("No tvx file");
    }
  }

commonMethod: 
(startLine=303 endLine=320 srcPath=/home/sonia/NewExperiment/luceneFilter/00347/src/java/org/apache/lucene/index/TermVectorsReader.java)
  final private long[] readTvfPointers(int fieldCount) throws IOException {
    // Compute position in the tvf file
    long position;
    if (format >= FORMAT_VERSION2)
      position = tvx.readLong();
    else
      position = tvd.readVLong();

    long[] tvfPointers = new long[fieldCount];
    tvfPointers[0] = position;

    for (int i = 1; i < fieldCount; i++) {
      position += tvd.readVLong();
      tvfPointers[i] = position;
    }

    return tvfPointers;
  }


, Instance #
frags: 
(startLine=211 endLine=233 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }

(startLine=251 endLine=274 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }

commonMethod: 
(startLine=128 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00347/src/java/org/apache/lucene/index/TermVectorsReader.java)
  final private void seekTvx(final int docNum) throws IOException {
    if (format < FORMAT_VERSION2)
      tvx.seek((docNum + docStoreOffset) * 8L + FORMAT_SIZE);
    else
      tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
  }


, Instance #
frags: 
(startLine=211 endLine=233 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }

(startLine=251 endLine=274 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }

commonMethod: 
(startLine=285 endLine=299 srcPath=/home/sonia/NewExperiment/luceneFilter/00347/src/java/org/apache/lucene/index/TermVectorsReader.java)
  final private String[] readFields(int fieldCount) throws IOException {
    int number = 0;
    String[] fields = new String[fieldCount];

    for (int i = 0; i < fieldCount; i++) {
      if (format >= FORMAT_VERSION)
        number = tvd.readVInt();
      else
        number += tvd.readVInt();

      fields[i] = fieldInfos.fieldName(number);
    }

    return fields;
  }


, Instance #
frags: 
(startLine=211 endLine=233 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }

(startLine=251 endLine=274 srcPath=/home/sonia/NewExperiment/luceneFilter/00346/src/java/org/apache/lucene/index/TermVectorsReader.java)
      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        // Compute position in the tvf file
        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        mapper.setDocumentNumber(docNumber);
        readTermVectors(fields, tvfPointers, mapper);
      }

commonMethod: 
(startLine=303 endLine=320 srcPath=/home/sonia/NewExperiment/luceneFilter/00347/src/java/org/apache/lucene/index/TermVectorsReader.java)
  final private long[] readTvfPointers(int fieldCount) throws IOException {
    // Compute position in the tvf file
    long position;
    if (format >= FORMAT_VERSION2)
      position = tvx.readLong();
    else
      position = tvd.readVLong();

    long[] tvfPointers = new long[fieldCount];
    tvfPointers[0] = position;

    for (int i = 1; i < fieldCount; i++) {
      position += tvd.readVLong();
      tvfPointers[i] = position;
    }

    return tvfPointers;
  }


, Instance #
frags: 
(startLine=120 endLine=128 srcPath=/home/sonia/NewExperiment/luceneFilter/00369/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java)
    {
        BooleanFilter booleanFilter = new BooleanFilter();
        booleanFilter.add(new FilterClause(getRangeFilter("price","010", "020"),BooleanClause.Occur.SHOULD));
        booleanFilter.add(new FilterClause(getRangeFilter("price","020", "030"),BooleanClause.Occur.SHOULD));
        booleanFilter.add(new FilterClause(getTermsFilter("accessRights", "admin"),BooleanClause.Occur.MUST));
        booleanFilter.add(new FilterClause(getRangeFilter("date","20040101", "20041231"),BooleanClause.Occur.MUST));
        BitSet bits = booleanFilter.bits(reader);
        assertEquals("Shoulds Ored but MUSTs ANDED",1,bits.cardinality());
    }

(startLine=130 endLine=138 srcPath=/home/sonia/NewExperiment/luceneFilter/00369/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java)
    {
        BooleanFilter booleanFilter = new BooleanFilter();
        booleanFilter.add(new FilterClause(getRangeFilter("price","030", "040"),BooleanClause.Occur.SHOULD));
        booleanFilter.add(new FilterClause(getTermsFilter("accessRights", "admin"),BooleanClause.Occur.MUST));
        booleanFilter.add(new FilterClause(getRangeFilter("date","20050101", "20051231"),BooleanClause.Occur.MUST));
        booleanFilter.add(new FilterClause(getTermsFilter("inStock","N"),BooleanClause.Occur.MUST_NOT));
        BitSet bits = booleanFilter.bits(reader);
        assertEquals("Shoulds Ored but MUSTs ANDED and MustNot",0,bits.cardinality());
    }

commonMethod: 
(startLine=90 endLine=99 srcPath=/home/sonia/NewExperiment/luceneFilter/00370/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java)
        private void tstFilterCard(String mes, int expected, Filter filt)
        throws Throwable
        {
          DocIdSetIterator disi = filt.getDocIdSet(reader).iterator();
          int actual = 0;
          while (disi.next()) {
            actual++;
          }
          assertEquals(mes, expected, actual);
        }


, Instance #
frags: 
(startLine=564 endLine=688 srcPath=/home/sonia/NewExperiment/luceneFilter/00423/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
{
   int startsAt = 0;
   jjnewStateCnt = 7;
   int i = 1;
   jjstateSet[0] = startState;
   int kind = 0x7fffffff;
   for (;;)
   {
      if (++jjround == 0x7fffffff)
         ReInitRounds();
      if (curChar < 64)
      {
         long l = 1L << curChar;
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
                  if ((0xfffffffeffffffffL & l) != 0L)
                  {
                     if (kind > 33)
                        kind = 33;
                     jjCheckNAdd(6);
                  }
                  if ((0x100002600L & l) != 0L)
                  {
                     if (kind > 7)
                        kind = 7;
                  }
                  else if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 1:
                  if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 2:
                  if ((0xfffffffbffffffffL & l) != 0L)
                     jjCheckNAddStates(16, 18);
                  break;
               case 3:
                  if (curChar == 34)
                     jjCheckNAddStates(16, 18);
                  break;
               case 5:
                  if (curChar == 34 && kind > 32)
                     kind = 32;
                  break;
               case 6:
                  if ((0xfffffffeffffffffL & l) == 0L)
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else if (curChar < 128)
      {
         long l = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if ((0xdfffffffffffffffL & l) == 0L)
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  jjAddStates(16, 18);
                  break;
               case 4:
                  if (curChar == 92)
                     jjstateSet[jjnewStateCnt++] = 3;
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      if (kind != 0x7fffffff)
      {
         jjmatchedKind = kind;
         jjmatchedPos = curPos;
         kind = 0x7fffffff;
      }
      ++curPos;
      if ((i = jjnewStateCnt) == (startsAt = 7 - (jjnewStateCnt = startsAt)))
         return curPos;
      try { curChar = input_stream.readChar(); }
      catch(java.io.IOException e) { return curPos; }
   }
}

(startLine=830 endLine=954 srcPath=/home/sonia/NewExperiment/luceneFilter/00423/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
{
   int startsAt = 0;
   jjnewStateCnt = 7;
   int i = 1;
   jjstateSet[0] = startState;
   int kind = 0x7fffffff;
   for (;;)
   {
      if (++jjround == 0x7fffffff)
         ReInitRounds();
      if (curChar < 64)
      {
         long l = 1L << curChar;
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
                  if ((0xfffffffeffffffffL & l) != 0L)
                  {
                     if (kind > 29)
                        kind = 29;
                     jjCheckNAdd(6);
                  }
                  if ((0x100002600L & l) != 0L)
                  {
                     if (kind > 7)
                        kind = 7;
                  }
                  else if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 1:
                  if (curChar == 34)
                     jjCheckNAddTwoStates(2, 4);
                  break;
               case 2:
                  if ((0xfffffffbffffffffL & l) != 0L)
                     jjCheckNAddStates(16, 18);
                  break;
               case 3:
                  if (curChar == 34)
                     jjCheckNAddStates(16, 18);
                  break;
               case 5:
                  if (curChar == 34 && kind > 28)
                     kind = 28;
                  break;
               case 6:
                  if ((0xfffffffeffffffffL & l) == 0L)
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else if (curChar < 128)
      {
         long l = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if ((0xffffffffdfffffffL & l) == 0L)
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  jjAddStates(16, 18);
                  break;
               case 4:
                  if (curChar == 92)
                     jjstateSet[jjnewStateCnt++] = 3;
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      else
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }
      if (kind != 0x7fffffff)
      {
         jjmatchedKind = kind;
         jjmatchedPos = curPos;
         kind = 0x7fffffff;
      }
      ++curPos;
      if ((i = jjnewStateCnt) == (startsAt = 7 - (jjnewStateCnt = startsAt)))
         return curPos;
      try { curChar = input_stream.readChar(); }
      catch(java.io.IOException e) { return curPos; }
   }
}

commonMethod: 
(startLine=1019 endLine=1030 srcPath=/home/sonia/NewExperiment/luceneFilter/00424/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
private static final boolean jjCanMove_1(int hiByte, int i1, int i2, long l1, long l2)
{
   switch(hiByte)
   {
      case 0:
         return ((jjbitVec3[i2] & l2) != 0L);
      default :
         if ((jjbitVec1[i1] & l1) != 0L)
            return true;
         return false;
   }
}


, Instance #
frags: 
(startLine=650 endLine=675 srcPath=/home/sonia/NewExperiment/luceneFilter/00423/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }

(startLine=916 endLine=941 srcPath=/home/sonia/NewExperiment/luceneFilter/00423/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
      {
         int hiByte = (int)(curChar >> 8);
         int i1 = hiByte >> 6;
         long l1 = 1L << (hiByte & 077);
         int i2 = (curChar & 0xff) >> 6;
         long l2 = 1L << (curChar & 077);
         do
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);
      }

commonMethod: 
(startLine=1019 endLine=1030 srcPath=/home/sonia/NewExperiment/luceneFilter/00424/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
private static final boolean jjCanMove_1(int hiByte, int i1, int i2, long l1, long l2)
{
   switch(hiByte)
   {
      case 0:
         return ((jjbitVec3[i2] & l2) != 0L);
      default :
         if ((jjbitVec1[i1] & l1) != 0L)
            return true;
         return false;
   }
}


, Instance #
frags: 
(startLine=657 endLine=674 srcPath=/home/sonia/NewExperiment/luceneFilter/00423/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 33)
                     kind = 33;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);

(startLine=923 endLine=940 srcPath=/home/sonia/NewExperiment/luceneFilter/00423/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
         {
            switch(jjstateSet[--i])
            {
               case 0:
               case 6:
                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
                     break;
                  if (kind > 29)
                     kind = 29;
                  jjCheckNAdd(6);
                  break;
               case 2:
                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
                     jjAddStates(16, 18);
                  break;
               default : break;
            }
         } while(i != startsAt);

commonMethod: 
(startLine=1019 endLine=1030 srcPath=/home/sonia/NewExperiment/luceneFilter/00424/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java)
private static final boolean jjCanMove_1(int hiByte, int i1, int i2, long l1, long l2)
{
   switch(hiByte)
   {
      case 0:
         return ((jjbitVec3[i2] & l2) != 0L);
      default :
         if ((jjbitVec1[i1] & l1) != 0L)
            return true;
         return false;
   }
}


, Instance #
frags: 
(startLine=193 endLine=205 srcPath=/home/sonia/NewExperiment/luceneFilter/00437/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
      } else {
        // We must shift the docIDs
        hc = new MultiReaderHitCollector() {
            private int docBase;
            public void collect(int doc, float score) {
              results.collect(doc + docBase + start, score);
            }

            public void setNextReader(IndexReader reader, int docBase) {
              this.docBase = docBase;
            }
          };
      }

(startLine=275 endLine=287 srcPath=/home/sonia/NewExperiment/luceneFilter/00437/src/java/org/apache/lucene/search/MultiSearcher.java)
      } else {
        // We must shift the docIDs
        hc = new MultiReaderHitCollector() {
            private int docBase;
            public void collect(int doc, float score) {
              results.collect(doc + docBase + start, score);
            }

            public void setNextReader(IndexReader reader, int docBase) {
              this.docBase = docBase;
            }
          };
      }

commonMethod: 
(startLine=127 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00438/src/java/org/apache/lucene/search/Collector.java)
  /**
   * Called before successive calls to {@link #collect(int)}. Implementations
   * that need the score of the current document (passed-in to
   * {@link #collect(int)}), should save the passed-in Scorer and call
   * scorer.score() when needed.
   */
  public abstract void setScorer(Scorer scorer) throws IOException;


, Instance #
frags: 
(startLine=301 endLine=310 srcPath=/home/sonia/NewExperiment/luceneFilter/00442/src/java/org/apache/lucene/index/MultiReader.java)
    throws IOException {
    ensureOpen();
    byte[] bytes = (byte[])normsCache.get(field);
    if (bytes==null && !hasNorms(field)) bytes=fakeNorms();
    if (bytes != null)                            // cache hit
      System.arraycopy(bytes, 0, result, offset, maxDoc());

    for (int i = 0; i < subReaders.length; i++)      // read from segments
      subReaders[i].norms(field, result, offset + starts[i]);
  }

(startLine=410 endLine=419 srcPath=/home/sonia/NewExperiment/luceneFilter/00442/src/java/org/apache/lucene/index/MultiSegmentReader.java)
    throws IOException {
    ensureOpen();
    byte[] bytes = (byte[])normsCache.get(field);
    if (bytes==null && !hasNorms(field)) bytes=fakeNorms();
    if (bytes != null)                            // cache hit
      System.arraycopy(bytes, 0, result, offset, maxDoc());

    for (int i = 0; i < subReaders.length; i++)      // read from segments
      subReaders[i].norms(field, result, offset + starts[i]);
  }

commonMethod: 
(startLine=397 endLine=413 srcPath=/home/sonia/NewExperiment/luceneFilter/00443/src/java/org/apache/lucene/search/Similarity.java)
  /** Encodes a normalization factor for storage in an index.
   *
   * <p>The encoding uses a three-bit mantissa, a five-bit exponent, and
   * the zero-exponent point at 15, thus
   * representing values from around 7x10^9 to 2x10^-9 with about one
   * significant decimal digit of accuracy.  Zero is also represented.
   * Negative numbers are rounded up to zero.  Values too large to represent
   * are rounded down to the largest representable value.  Positive values too
   * small to represent are rounded up to the smallest positive representable
   * value.
   *
   * @see org.apache.lucene.document.Field#setBoost(float)
   * @see org.apache.lucene.util.SmallFloat
   */
  public static byte encodeNorm(float f) {
    return SmallFloat.floatToByte315(f);
  }


, Instance #
frags: 
(startLine=2331 endLine=2362 srcPath=/home/sonia/NewExperiment/luceneFilter/00443/src/java/org/apache/lucene/index/IndexWriter.java)
  public void addDocument(Document doc, Analyzer analyzer) throws CorruptIndexException, IOException {
    ensureOpen();
    boolean doFlush = false;
    boolean success = false;
    try {
      try {
        doFlush = docWriter.addDocument(doc, analyzer);
        success = true;
      } finally {
        if (!success) {

          if (infoStream != null)
            message("hit exception adding document");

          synchronized (this) {
            // If docWriter has some aborted files that were
            // never incref'd, then we clean them up here
            if (docWriter != null) {
              final Collection files = docWriter.abortedFiles();
              if (files != null)
                deleter.deleteNewFiles(files);
            }
          }
        }
      }
      if (doFlush)
        flush(true, false, false);
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

(startLine=2491 endLine=2520 srcPath=/home/sonia/NewExperiment/luceneFilter/00443/src/java/org/apache/lucene/index/IndexWriter.java)
      throws CorruptIndexException, IOException {
    ensureOpen();
    try {
      boolean doFlush = false;
      boolean success = false;
      try {
        doFlush = docWriter.updateDocument(term, doc, analyzer);
        success = true;
      } finally {
        if (!success) {

          if (infoStream != null)
            message("hit exception updating document");

          synchronized (this) {
            // If docWriter has some aborted files that were
            // never incref'd, then we clean them up here
            final Collection files = docWriter.abortedFiles();
            if (files != null)
              deleter.deleteNewFiles(files);
          }
        }
      }
      if (doFlush)
        flush(true, false, false);
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    }
  }

commonMethod: 
(startLine=5444 endLine=5450 srcPath=/home/sonia/NewExperiment/luceneFilter/00444/src/java/org/apache/lucene/index/IndexWriter.java)
  private void handleOOM(OutOfMemoryError oom, String location) {
    if (infoStream != null) {
      message("hit OutOfMemoryError inside " + location);
    }
    hitOOM = true;
    throw oom;
  }


, Instance #
frags: 
(startLine=743 endLine=775 srcPath=/home/sonia/NewExperiment/luceneFilter/00459/src/test/org/apache/lucene/index/TestIndexWriter.java)
    public void testCreateWithReader() throws IOException {
        String tempDir = System.getProperty("java.io.tmpdir");
        if (tempDir == null)
            throw new IOException("java.io.tmpdir undefined, cannot run test");
        File indexDir = new File(tempDir, "lucenetestindexwriter");

        try {
          Directory dir = FSDirectory.open(indexDir);

          // add one document & close writer
          IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
          addDoc(writer);
          writer.close();

          // now open reader:
          IndexReader reader = IndexReader.open(dir);
          assertEquals("should be one document", reader.numDocs(), 1);

          // now open index for create:
          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
          assertEquals("should be zero documents", writer.docCount(), 0);
          addDoc(writer);
          writer.close();

          assertEquals("should be one document", reader.numDocs(), 1);
          IndexReader reader2 = IndexReader.open(dir);
          assertEquals("should be one document", reader2.numDocs(), 1);
          reader.close();
          reader2.close();
        } finally {
          rmDir(indexDir);
        }
    }

(startLine=780 endLine=809 srcPath=/home/sonia/NewExperiment/luceneFilter/00459/src/test/org/apache/lucene/index/TestIndexWriter.java)
    public void testCreateWithReader2() throws IOException {
        String tempDir = System.getProperty("java.io.tmpdir");
        if (tempDir == null)
            throw new IOException("java.io.tmpdir undefined, cannot run test");
        File indexDir = new File(tempDir, "lucenetestindexwriter");
        try {
          // add one document & close writer
          IndexWriter writer = new IndexWriter(indexDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
          addDoc(writer);
          writer.close();

          // now open reader:
          IndexReader reader = IndexReader.open(indexDir);
          assertEquals("should be one document", reader.numDocs(), 1);

          // now open index for create:
          writer = new IndexWriter(indexDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
          assertEquals("should be zero documents", writer.docCount(), 0);
          addDoc(writer);
          writer.close();

          assertEquals("should be one document", reader.numDocs(), 1);
          IndexReader reader2 = IndexReader.open(indexDir);
          assertEquals("should be one document", reader2.numDocs(), 1);
          reader.close();
          reader2.close();
        } finally {
          rmDir(indexDir);
        }
    }

(startLine=813 endLine=844 srcPath=/home/sonia/NewExperiment/luceneFilter/00459/src/test/org/apache/lucene/index/TestIndexWriter.java)
    public void testCreateWithReader3() throws IOException {
        String tempDir = System.getProperty("tempDir");
        if (tempDir == null)
            throw new IOException("java.io.tmpdir undefined, cannot run test");

        String dirName = tempDir + "/lucenetestindexwriter";
        try {

          // add one document & close writer
          IndexWriter writer = new IndexWriter(dirName, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
          addDoc(writer);
          writer.close();

          // now open reader:
          IndexReader reader = IndexReader.open(dirName);
          assertEquals("should be one document", reader.numDocs(), 1);

          // now open index for create:
          writer = new IndexWriter(dirName, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
          assertEquals("should be zero documents", writer.docCount(), 0);
          addDoc(writer);
          writer.close();

          assertEquals("should be one document", reader.numDocs(), 1);
          IndexReader reader2 = IndexReader.open(dirName);
          assertEquals("should be one document", reader2.numDocs(), 1);
          reader.close();
          reader2.close();
        } finally {
          rmDir(new File(dirName));
        }
    }

commonMethod: 
(startLine=33 endLine=40 srcPath=/home/sonia/NewExperiment/luceneFilter/00460/src/test/org/apache/lucene/util/_TestUtil.java)
  /** Returns temp dir, containing String arg in its name;
   *  does not create the directory. */
  public static File getTempDir(String desc) {
    String tempDir = System.getProperty("java.io.tmpdir");
    if (tempDir == null)
      throw new RuntimeException("java.io.tmpdir undefined, cannot run test");
    return new File(tempDir, desc + "." + new Random().nextLong());
  }


, Instance #
frags: 
(startLine=60 endLine=75 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java)
    public void testUnigrams() throws Exception {
      NGramTokenFilter filter = new NGramTokenFilter(input, 1, 1);

      final Token reusableToken = new Token();
        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
            tokens.add(nextToken.toString());
//          System.out.println(token.term());
//          System.out.println(token);
//          Thread.sleep(1000);
        }

        assertEquals(5, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
        assertEquals(exp, tokens);
    }

(startLine=77 endLine=91 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java)
    public void testBigrams() throws Exception {
      NGramTokenFilter filter = new NGramTokenFilter(input, 2, 2);
      final Token reusableToken = new Token();
        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
            tokens.add(nextToken.toString());
//          System.out.println(token.term());
//          System.out.println(token);
//          Thread.sleep(1000);
        }

        assertEquals(4, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(ab,0,2)"); exp.add("(bc,1,3)"); exp.add("(cd,2,4)"); exp.add("(de,3,5)");
        assertEquals(exp, tokens);
    }

commonMethod: 
(startLine=59 endLine=66 srcPath=/home/sonia/NewExperiment/luceneFilter/00499/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java)
    private void checkStream(TokenStream stream, String[] exp) throws IOException {
      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
      for (int i = 0; i < exp.length; i++) {
        assertTrue(stream.incrementToken());
        assertEquals(exp[i], termAtt.toString());
      }
      assertFalse(stream.incrementToken());
    }


, Instance #
frags: 
(startLine=58 endLine=73 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java)
    public void testUnigrams() throws Exception {
        NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
        
        final Token reusableToken = new Token();
        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
          tokens.add(nextToken.toString());
//        System.out.println(token.term());
//        System.out.println(token);
//        Thread.sleep(1000);
      }

        assertEquals(5, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
        assertEquals(exp, tokens);
    }

(startLine=75 endLine=89 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java)
    public void testBigrams() throws Exception {
        NGramTokenizer tokenizer = new NGramTokenizer(input, 2, 2);
        final Token reusableToken = new Token();
        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
          tokens.add(nextToken.toString());
//        System.out.println(token.term());
//        System.out.println(token);
//        Thread.sleep(1000);
      }

        assertEquals(4, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(ab,0,2)"); exp.add("(bc,1,3)"); exp.add("(cd,2,4)"); exp.add("(de,3,5)");
        assertEquals(exp, tokens);
    }

commonMethod: 
(startLine=59 endLine=66 srcPath=/home/sonia/NewExperiment/luceneFilter/00499/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java)
    private void checkStream(TokenStream stream, String[] exp) throws IOException {
      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
      for (int i = 0; i < exp.length; i++) {
        assertTrue(stream.incrementToken());
        assertEquals(exp[i], termAtt.toString());
      }
      assertFalse(stream.incrementToken());
    }


, Instance #
frags: 
(startLine=77 endLine=91 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java)
    public void testBigrams() throws Exception {
      NGramTokenFilter filter = new NGramTokenFilter(input, 2, 2);
      final Token reusableToken = new Token();
        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
            tokens.add(nextToken.toString());
//          System.out.println(token.term());
//          System.out.println(token);
//          Thread.sleep(1000);
        }

        assertEquals(4, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(ab,0,2)"); exp.add("(bc,1,3)"); exp.add("(cd,2,4)"); exp.add("(de,3,5)");
        assertEquals(exp, tokens);
    }

(startLine=60 endLine=75 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java)
    public void testUnigrams() throws Exception {
      NGramTokenFilter filter = new NGramTokenFilter(input, 1, 1);

      final Token reusableToken = new Token();
        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
            tokens.add(nextToken.toString());
//          System.out.println(token.term());
//          System.out.println(token);
//          Thread.sleep(1000);
        }

        assertEquals(5, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
        assertEquals(exp, tokens);
    }

commonMethod: 
(startLine=59 endLine=66 srcPath=/home/sonia/NewExperiment/luceneFilter/00499/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java)
    private void checkStream(TokenStream stream, String[] exp) throws IOException {
      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
      for (int i = 0; i < exp.length; i++) {
        assertTrue(stream.incrementToken());
        assertEquals(exp[i], termAtt.toString());
      }
      assertFalse(stream.incrementToken());
    }


, Instance #
frags: 
(startLine=75 endLine=89 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java)
    public void testBigrams() throws Exception {
        NGramTokenizer tokenizer = new NGramTokenizer(input, 2, 2);
        final Token reusableToken = new Token();
        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
          tokens.add(nextToken.toString());
//        System.out.println(token.term());
//        System.out.println(token);
//        Thread.sleep(1000);
      }

        assertEquals(4, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(ab,0,2)"); exp.add("(bc,1,3)"); exp.add("(cd,2,4)"); exp.add("(de,3,5)");
        assertEquals(exp, tokens);
    }

(startLine=58 endLine=73 srcPath=/home/sonia/NewExperiment/luceneFilter/00498/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java)
    public void testUnigrams() throws Exception {
        NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
        
        final Token reusableToken = new Token();
        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
          tokens.add(nextToken.toString());
//        System.out.println(token.term());
//        System.out.println(token);
//        Thread.sleep(1000);
      }

        assertEquals(5, tokens.size());
        ArrayList exp = new ArrayList();
        exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
        assertEquals(exp, tokens);
    }

commonMethod: 
(startLine=59 endLine=66 srcPath=/home/sonia/NewExperiment/luceneFilter/00499/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java)
    private void checkStream(TokenStream stream, String[] exp) throws IOException {
      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
      for (int i = 0; i < exp.length; i++) {
        assertTrue(stream.incrementToken());
        assertEquals(exp[i], termAtt.toString());
      }
      assertFalse(stream.incrementToken());
    }


, Instance #
frags: 
(startLine=58 endLine=95 srcPath=/home/sonia/NewExperiment/luceneFilter/00591/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
    throws IOException {
    HitQueue hq = new HitQueue(nDocs, false);
    int totalHits = 0;
    MultiSearcherThread[] msta =
      new MultiSearcherThread[searchables.length];
    for (int i = 0; i < searchables.length; i++) { // search each searchable
      // Assume not too many searchables and cost of creating a thread is by far inferior to a search
      msta[i] = new MultiSearcherThread(searchables[i], weight, filter, nDocs,
          hq, i, starts, "MultiSearcher thread #" + (i + 1));
      msta[i].start();
    }

    for (int i = 0; i < searchables.length; i++) {
      try {
        msta[i].join();
      } catch (InterruptedException ie) {
        // In 3.0 we will change this to throw
        // InterruptedException instead
        Thread.currentThread().interrupt();
        throw new RuntimeException(ie);
      }
      IOException ioe = msta[i].getIOException();
      if (ioe == null) {
        totalHits += msta[i].hits();
      } else {
        // if one search produced an IOException, rethrow it
        throw ioe;
      }
    }

    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size() - 1; i >= 0; i--) // put docs in array
      scoreDocs[i] = hq.pop();

    float maxScore = (totalHits==0) ? Float.NEGATIVE_INFINITY : scoreDocs[0].score;
    
    return new TopDocs(totalHits, scoreDocs, maxScore);
  }

(startLine=104 endLine=142 srcPath=/home/sonia/NewExperiment/luceneFilter/00591/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
    throws IOException {
    // don't specify the fields - we'll wait to do this until we get results
    FieldDocSortedHitQueue hq = new FieldDocSortedHitQueue (null, nDocs);
    int totalHits = 0;
    MultiSearcherThread[] msta = new MultiSearcherThread[searchables.length];
    for (int i = 0; i < searchables.length; i++) { // search each searchable
      // Assume not too many searchables and cost of creating a thread is by far inferior to a search
      msta[i] = new MultiSearcherThread(searchables[i], weight, filter, nDocs,
          hq, sort, i, starts, "MultiSearcher thread #" + (i + 1));
      msta[i].start();
    }

    float maxScore=Float.NEGATIVE_INFINITY;
    
    for (int i = 0; i < searchables.length; i++) {
      try {
        msta[i].join();
      } catch (InterruptedException ie) {
        // In 3.0 we will change this to throw
        // InterruptedException instead
        Thread.currentThread().interrupt();
        throw new RuntimeException(ie);
      }
      IOException ioe = msta[i].getIOException();
      if (ioe == null) {
        totalHits += msta[i].hits();
        maxScore=Math.max(maxScore, msta[i].getMaxScore());
      } else {
        // if one search produced an IOException, rethrow it
        throw ioe;
      }
    }

    ScoreDoc[] scoreDocs = new ScoreDoc[hq.size()];
    for (int i = hq.size() - 1; i >= 0; i--) // put docs in array
      scoreDocs[i] = hq.pop();

    return new TopFieldDocs(totalHits, scoreDocs, hq.getFields(), maxScore);
  }

commonMethod: 
(startLine=181 endLine=196 srcPath=/home/sonia/NewExperiment/luceneFilter/00592/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
  private <T> void foreach(Function<T> func, List<Future<T>> list) throws IOException{
    for (Future<T> future : list) {
      try{
        func.apply(future.get());
      } catch (ExecutionException e) {
        if (e.getCause() instanceof IOException)
          throw (IOException) e.getCause();
        throw new RuntimeException(e.getCause());
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
        // In 3.0 we will change this to throw
        // InterruptedException instead
        throw new RuntimeException(e);
      }
    }
  }


, Instance #
frags: 
(startLine=70 endLine=86 srcPath=/home/sonia/NewExperiment/luceneFilter/00591/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
    for (int i = 0; i < searchables.length; i++) {
      try {
        msta[i].join();
      } catch (InterruptedException ie) {
        // In 3.0 we will change this to throw
        // InterruptedException instead
        Thread.currentThread().interrupt();
        throw new RuntimeException(ie);
      }
      IOException ioe = msta[i].getIOException();
      if (ioe == null) {
        totalHits += msta[i].hits();
      } else {
        // if one search produced an IOException, rethrow it
        throw ioe;
      }
    }

(startLine=118 endLine=135 srcPath=/home/sonia/NewExperiment/luceneFilter/00591/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
    for (int i = 0; i < searchables.length; i++) {
      try {
        msta[i].join();
      } catch (InterruptedException ie) {
        // In 3.0 we will change this to throw
        // InterruptedException instead
        Thread.currentThread().interrupt();
        throw new RuntimeException(ie);
      }
      IOException ioe = msta[i].getIOException();
      if (ioe == null) {
        totalHits += msta[i].hits();
        maxScore=Math.max(maxScore, msta[i].getMaxScore());
      } else {
        // if one search produced an IOException, rethrow it
        throw ioe;
      }
    }

commonMethod: 
(startLine=181 endLine=196 srcPath=/home/sonia/NewExperiment/luceneFilter/00592/src/java/org/apache/lucene/search/ParallelMultiSearcher.java)
  private <T> void foreach(Function<T> func, List<Future<T>> list) throws IOException{
    for (Future<T> future : list) {
      try{
        func.apply(future.get());
      } catch (ExecutionException e) {
        if (e.getCause() instanceof IOException)
          throw (IOException) e.getCause();
        throw new RuntimeException(e.getCause());
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
        // In 3.0 we will change this to throw
        // InterruptedException instead
        throw new RuntimeException(e);
      }
    }
  }


, Instance #
frags: 
(startLine=112 endLine=165 srcPath=/home/sonia/NewExperiment/luceneFilter/00627/src/java/org/apache/lucene/util/UnicodeUtil.java)
  public static void UTF16toUTF8(final char[] source, final int offset, UTF8Result result) {

    int upto = 0;
    int i = offset;
    byte[] out = result.result;

    while(true) {
      
      final int code = (int) source[i++];

      if (upto+4 > out.length) {
        byte[] newOut = new byte[2*out.length];
        assert newOut.length >= upto+4;
        System.arraycopy(out, 0, newOut, 0, upto);
        result.result = out = newOut;
      }
      if (code < 0x80)
        out[upto++] = (byte) code;
      else if (code < 0x800) {
        out[upto++] = (byte) (0xC0 | (code >> 6));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else if (code < 0xD800 || code > 0xDFFF) {
        if (code == 0xffff)
          // END
          break;
        out[upto++] = (byte)(0xE0 | (code >> 12));
        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else {
        // surrogate pair
        // confirm valid high surrogate
        if (code < 0xDC00 && source[i] != 0xffff) {
          int utf32 = (int) source[i];
          // confirm valid low surrogate and write pair
          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
            i++;
            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
            continue;
          }
        }
        // replace unpaired surrogate or out-of-order low surrogate
        // with substitution character
        out[upto++] = (byte) 0xEF;
        out[upto++] = (byte) 0xBF;
        out[upto++] = (byte) 0xBD;
      }
    }
    //assert matches(source, offset, i-offset-1, out, upto);
    result.length = upto;
  }

(startLine=170 endLine=221 srcPath=/home/sonia/NewExperiment/luceneFilter/00627/src/java/org/apache/lucene/util/UnicodeUtil.java)
  public static void UTF16toUTF8(final char[] source, final int offset, final int length, UTF8Result result) {

    int upto = 0;
    int i = offset;
    final int end = offset + length;
    byte[] out = result.result;

    while(i < end) {
      
      final int code = (int) source[i++];

      if (upto+4 > out.length) {
        byte[] newOut = new byte[2*out.length];
        assert newOut.length >= upto+4;
        System.arraycopy(out, 0, newOut, 0, upto);
        result.result = out = newOut;
      }
      if (code < 0x80)
        out[upto++] = (byte) code;
      else if (code < 0x800) {
        out[upto++] = (byte) (0xC0 | (code >> 6));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else if (code < 0xD800 || code > 0xDFFF) {
        out[upto++] = (byte)(0xE0 | (code >> 12));
        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else {
        // surrogate pair
        // confirm valid high surrogate
        if (code < 0xDC00 && i < end && source[i] != 0xffff) {
          int utf32 = (int) source[i];
          // confirm valid low surrogate and write pair
          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
            i++;
            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
            continue;
          }
        }
        // replace unpaired surrogate or out-of-order low surrogate
        // with substitution character
        out[upto++] = (byte) 0xEF;
        out[upto++] = (byte) 0xBF;
        out[upto++] = (byte) 0xBD;
      }
    }
    //assert matches(source, offset, length, out, upto);
    result.length = upto;
  }

(startLine=226 endLine=275 srcPath=/home/sonia/NewExperiment/luceneFilter/00627/src/java/org/apache/lucene/util/UnicodeUtil.java)
  public static void UTF16toUTF8(final String s, final int offset, final int length, UTF8Result result) {
    final int end = offset + length;

    byte[] out = result.result;

    int upto = 0;
    for(int i=offset;i<end;i++) {
      final int code = (int) s.charAt(i);

      if (upto+4 > out.length) {
        byte[] newOut = new byte[2*out.length];
        assert newOut.length >= upto+4;
        System.arraycopy(out, 0, newOut, 0, upto);
        result.result = out = newOut;
      }
      if (code < 0x80)
        out[upto++] = (byte) code;
      else if (code < 0x800) {
        out[upto++] = (byte) (0xC0 | (code >> 6));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else if (code < 0xD800 || code > 0xDFFF) {
        out[upto++] = (byte)(0xE0 | (code >> 12));
        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
        out[upto++] = (byte)(0x80 | (code & 0x3F));
      } else {
        // surrogate pair
        // confirm valid high surrogate
        if (code < 0xDC00 && (i < end-1)) {
          int utf32 = (int) s.charAt(i+1);
          // confirm valid low surrogate and write pair
          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
            i++;
            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
            continue;
          }
        }
        // replace unpaired surrogate or out-of-order low surrogate
        // with substitution character
        out[upto++] = (byte) 0xEF;
        out[upto++] = (byte) 0xBF;
        out[upto++] = (byte) 0xBD;
      }
    }
    //assert matches(s, offset, length, out, upto);
    result.length = upto;
  }

commonMethod: 
(startLine=269 endLine=276 srcPath=/home/sonia/NewExperiment/luceneFilter/00628/src/java/org/apache/lucene/util/ArrayUtil.java)
  public static byte[] grow(byte[] array, int minSize) {
    if (array.length < minSize) {
      byte[] newArray = new byte[oversize(minSize, 1)];
      System.arraycopy(array, 0, newArray, 0, array.length);
      return newArray;
    } else
      return array;
  }


, Instance #
frags: 
(startLine=237 endLine=244 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java)
  public void testSearchOldIndex() throws IOException {
    for(int i=0;i<oldNames.length;i++) {
      String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
      unzip(dirName, oldNames[i]);
      searchIndex(oldNames[i], oldNames[i]);
      rmDir(oldNames[i]);
    }
  }

(startLine=246 endLine=253 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java)
  public void testIndexOldIndexNoAdds() throws IOException {
    for(int i=0;i<oldNames.length;i++) {
      String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
      unzip(dirName, oldNames[i]);
      changeIndexNoAdds(oldNames[i]);
      rmDir(oldNames[i]);
    }
  }

(startLine=255 endLine=262 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java)
  public void testIndexOldIndex() throws IOException {
    for(int i=0;i<oldNames.length;i++) {
      String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
      unzip(dirName, oldNames[i]);
      changeIndexWithAdds(oldNames[i]);
      rmDir(oldNames[i]);
    }
  }

commonMethod: 
(startLine=259 endLine=269 srcPath=/home/sonia/NewExperiment/luceneFilter/00666/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java)
  /** Gets a resource from the classpath as {@link File}. This method should only be used,
   * if a real file is needed. To get a stream, code should prefer
   * {@link Class#getResourceAsStream} using {@code this.getClass()}.
   */
  protected File getDataFile(String name) throws IOException {
    try {
      return new File(this.getClass().getResource(name).toURI());
    } catch (Exception e) {
      throw new IOException("Cannot find resource: " + name);
    }
  }


, Instance #
frags: 
(startLine=39 endLine=64 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java)
  public void inform(ResourceLoader loader) {
    String commonWordFiles = args.get("words");
    ignoreCase = getBoolean("ignoreCase", false);

    if (commonWordFiles != null) {
      try {
        List<String> files = StrUtils.splitFileNames(commonWordFiles);
        if (commonWords == null && files.size() > 0) {
          // default stopwords list has 35 or so words, but maybe don't make it
          // that big to start
          commonWords = new CharArraySet(files.size() * 10, ignoreCase);
        }
        for (String file : files) {
          List<String> wlist = loader.getLines(file.trim());
          // TODO: once StopFilter.makeStopSet(List) method is available, switch
          // to using that so we can avoid a toArray() call
          commonWords.addAll(CommonGramsFilter.makeCommonSet((String[]) wlist
              .toArray(new String[0]), ignoreCase));
        }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
    } else {
      commonWords = (CharArraySet) StopAnalyzer.ENGLISH_STOP_WORDS_SET;
    }
  }

(startLine=40 endLine=62 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java)
  public void inform(ResourceLoader loader) {
    String commonWordFiles = args.get("words");
    ignoreCase = getBoolean("ignoreCase", false);

    if (commonWordFiles != null) {
      try {
        List<String> files = StrUtils.splitFileNames(commonWordFiles);
          if (commonWords == null && files.size() > 0){
            //default stopwords list has 35 or so words, but maybe don't make it that big to start
            commonWords = new CharArraySet(files.size() * 10, ignoreCase);
          }
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            //TODO: once StopFilter.makeStopSet(List) method is available, switch to using that so we can avoid a toArray() call
            commonWords.addAll(CommonGramsFilter.makeCommonSet((String[])wlist.toArray(new String[0]), ignoreCase));
          }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
    } else {
      commonWords = (CharArraySet) StopAnalyzer.ENGLISH_STOP_WORDS_SET;
    }
  }

(startLine=45 endLine=59 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/StopFilterFactory.java)
    if (stopWordFiles != null) {
      try {
        List<String> files = StrUtils.splitFileNames(stopWordFiles);
          if (stopWords == null && files.size() > 0){
            //default stopwords list has 35 or so words, but maybe don't make it that big to start
            stopWords = new CharArraySet(files.size() * 10, ignoreCase);
          }
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            stopWords.addAll(StopFilter.makeStopSet(wlist, ignoreCase));
          }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
    } else {

(startLine=44 endLine=63 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/KeepWordFilterFactory.java)
  public void inform(ResourceLoader loader) {
    String wordFiles = args.get("words");
    ignoreCase = getBoolean("ignoreCase", false);
    if (wordFiles != null) {
      try {
        List<String> files = StrUtils.splitFileNames(wordFiles);
        if (words == null && files.size() > 0){
          words = new CharArraySet(files.size() * 10, ignoreCase);
        }
        for (String file : files) {
          List<String> wlist = loader.getLines(file.trim());
          //TODO: once StopFilter.makeStopSet(List) method is available, switch to using that so we can avoid a toArray() call
          words.addAll(StopFilter.makeStopSet((String[]) wlist.toArray(new String[0]), ignoreCase));
        }
      }
      catch (IOException e) {
        throw new RuntimeException(e);
      }
    }
  }

commonMethod: 
(startLine=101 endLine=118 srcPath=/home/sonia/NewExperiment/luceneFilter/00666/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java)
  protected CharArraySet getWordSet(ResourceLoader loader,
      String wordFiles, boolean ignoreCase) throws IOException {
    assureMatchVersion();
    List<String> files = StrUtils.splitFileNames(wordFiles);
    CharArraySet words = null;
    if (files.size() > 0) {
      // default stopwords list has 35 or so words, but maybe don't make it that
      // big to start
      words = new CharArraySet(luceneMatchVersion, 
          files.size() * 10, ignoreCase);
      for (String file : files) {
        List<String> wlist = loader.getLines(file.trim());
        words.addAll(StopFilter.makeStopSet(luceneMatchVersion, wlist,
            ignoreCase));
      }
    }
    return words;
  }


, Instance #
frags: 
(startLine=41 endLine=64 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/EnglishPorterFilterFactory.java)
  public void inform(ResourceLoader loader) {
    String wordFiles = args.get(PROTECTED_TOKENS);
    if (wordFiles != null) {
      try {
        File protectedWordFiles = new File(wordFiles);
        if (protectedWordFiles.exists()) {
          List<String> wlist = loader.getLines(wordFiles);
          //This cast is safe in Lucene
          protectedWords = new CharArraySet(wlist, false);//No need to go through StopFilter as before, since it just uses a List internally
        } else  {
          List<String> files = StrUtils.splitFileNames(wordFiles);
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            if (protectedWords == null)
              protectedWords = new CharArraySet(wlist, false);
            else
              protectedWords.addAll(wlist);
          }
        }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
    }
  }

(startLine=50 endLine=73 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/SnowballPorterFilterFactory.java)
  public void inform(ResourceLoader loader) {
    String wordFiles = args.get(PROTECTED_TOKENS);
    if (wordFiles != null) {
      try {
        File protectedWordFiles = new File(wordFiles);
        if (protectedWordFiles.exists()) {
          List<String> wlist = loader.getLines(wordFiles);
          //This cast is safe in Lucene
          protectedWords = new CharArraySet(wlist, false);//No need to go through StopFilter as before, since it just uses a List internally
        } else  {
          List<String> files = StrUtils.splitFileNames(wordFiles);
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            if (protectedWords == null)
              protectedWords = new CharArraySet(wlist, false);
            else
              protectedWords.addAll(wlist);
          }
        }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
    }
  }

(startLine=39 endLine=62 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/WordDelimiterFilterFactory.java)
  public void inform(ResourceLoader loader) {
    String wordFiles = args.get(PROTECTED_TOKENS);
    if (wordFiles != null) {  
      try {
        File protectedWordFiles = new File(wordFiles);
        if (protectedWordFiles.exists()) {
          List<String> wlist = loader.getLines(wordFiles);
          //This cast is safe in Lucene
          protectedWords = new CharArraySet(wlist, false);//No need to go through StopFilter as before, since it just uses a List internally
        } else  {
          List<String> files = StrUtils.splitFileNames(wordFiles);
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            if (protectedWords == null)
              protectedWords = new CharArraySet(wlist, false);
            else
              protectedWords.addAll(wlist);
          }
        }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
    }
  }

commonMethod: 
(startLine=101 endLine=118 srcPath=/home/sonia/NewExperiment/luceneFilter/00666/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java)
  protected CharArraySet getWordSet(ResourceLoader loader,
      String wordFiles, boolean ignoreCase) throws IOException {
    assureMatchVersion();
    List<String> files = StrUtils.splitFileNames(wordFiles);
    CharArraySet words = null;
    if (files.size() > 0) {
      // default stopwords list has 35 or so words, but maybe don't make it that
      // big to start
      words = new CharArraySet(luceneMatchVersion, 
          files.size() * 10, ignoreCase);
      for (String file : files) {
        List<String> wlist = loader.getLines(file.trim());
        words.addAll(StopFilter.makeStopSet(luceneMatchVersion, wlist,
            ignoreCase));
      }
    }
    return words;
  }


, Instance #
frags: 
(startLine=50 endLine=59 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/EnglishPorterFilterFactory.java)
        } else  {
          List<String> files = StrUtils.splitFileNames(wordFiles);
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            if (protectedWords == null)
              protectedWords = new CharArraySet(wlist, false);
            else
              protectedWords.addAll(wlist);
          }
        }

(startLine=59 endLine=68 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/SnowballPorterFilterFactory.java)
        } else  {
          List<String> files = StrUtils.splitFileNames(wordFiles);
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            if (protectedWords == null)
              protectedWords = new CharArraySet(wlist, false);
            else
              protectedWords.addAll(wlist);
          }
        }

(startLine=48 endLine=57 srcPath=/home/sonia/NewExperiment/luceneFilter/00665/solr/src/java/org/apache/solr/analysis/WordDelimiterFilterFactory.java)
        } else  {
          List<String> files = StrUtils.splitFileNames(wordFiles);
          for (String file : files) {
            List<String> wlist = loader.getLines(file.trim());
            if (protectedWords == null)
              protectedWords = new CharArraySet(wlist, false);
            else
              protectedWords.addAll(wlist);
          }
        }

commonMethod: 
(startLine=101 endLine=118 srcPath=/home/sonia/NewExperiment/luceneFilter/00666/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java)
  protected CharArraySet getWordSet(ResourceLoader loader,
      String wordFiles, boolean ignoreCase) throws IOException {
    assureMatchVersion();
    List<String> files = StrUtils.splitFileNames(wordFiles);
    CharArraySet words = null;
    if (files.size() > 0) {
      // default stopwords list has 35 or so words, but maybe don't make it that
      // big to start
      words = new CharArraySet(luceneMatchVersion, 
          files.size() * 10, ignoreCase);
      for (String file : files) {
        List<String> wlist = loader.getLines(file.trim());
        words.addAll(StopFilter.makeStopSet(luceneMatchVersion, wlist,
            ignoreCase));
      }
    }
    return words;
  }


, Instance #
frags: 
(startLine=366 endLine=400 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      IntParser parser = (IntParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getInts(reader, field, DEFAULT_INT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getInts(reader, field, NUMERIC_UTILS_INT_PARSER);      
        }
      }
      int[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          if (retArray == null) // late init
            retArray = new int[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new int[reader.maxDoc()];
      return retArray;
    }

(startLine=424 endLine=458 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FloatParser parser = (FloatParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getFloats(reader, field, DEFAULT_FLOAT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getFloats(reader, field, NUMERIC_UTILS_FLOAT_PARSER);      
        }
    }
      float[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          if (retArray == null) // late init
            retArray = new float[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new float[reader.maxDoc()];
      return retArray;
    }

(startLine=479 endLine=512 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      String field = entry.field;
      FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getLongs(reader, field, DEFAULT_LONG_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getLongs(reader, field, NUMERIC_UTILS_LONG_PARSER);      
        }
      }
      long[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term(field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          if (retArray == null) // late init
            retArray = new long[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new long[reader.maxDoc()];
      return retArray;
    }

(startLine=534 endLine=568 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getDoubles(reader, field, DEFAULT_DOUBLE_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getDoubles(reader, field, NUMERIC_UTILS_DOUBLE_PARSER);      
        }
      }
      double[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          if (retArray == null) // late init
            retArray = new double[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new double[reader.maxDoc()];
      return retArray;
    }

commonMethod: 
(startLine=135 endLine=143 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  /**  This method may return null if the field does not exist.*/
  public static Terms getTerms(IndexReader r, String field) throws IOException {
    final Fields fields = getFields(r);
    if (fields == null) {
      return null;
    } else {
      return fields.terms(field);
    }
  }


, Instance #
frags: 
(startLine=366 endLine=400 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      IntParser parser = (IntParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getInts(reader, field, DEFAULT_INT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getInts(reader, field, NUMERIC_UTILS_INT_PARSER);      
        }
      }
      int[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          if (retArray == null) // late init
            retArray = new int[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new int[reader.maxDoc()];
      return retArray;
    }

(startLine=424 endLine=458 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FloatParser parser = (FloatParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getFloats(reader, field, DEFAULT_FLOAT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getFloats(reader, field, NUMERIC_UTILS_FLOAT_PARSER);      
        }
    }
      float[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          if (retArray == null) // late init
            retArray = new float[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new float[reader.maxDoc()];
      return retArray;
    }

(startLine=479 endLine=512 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      String field = entry.field;
      FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getLongs(reader, field, DEFAULT_LONG_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getLongs(reader, field, NUMERIC_UTILS_LONG_PARSER);      
        }
      }
      long[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term(field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          if (retArray == null) // late init
            retArray = new long[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new long[reader.maxDoc()];
      return retArray;
    }

(startLine=534 endLine=568 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getDoubles(reader, field, DEFAULT_DOUBLE_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getDoubles(reader, field, NUMERIC_UTILS_DOUBLE_PARSER);      
        }
      }
      double[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          if (retArray == null) // late init
            retArray = new double[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new double[reader.maxDoc()];
      return retArray;
    }

commonMethod: 
(startLine=99 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  public static Bits getDeletedDocs(IndexReader r) throws IOException {
    Bits result;
    if (r.hasDeletions()) {

      result = r.retrieveDelDocs();
      if (result == null) {

        final List<Bits> bits = new ArrayList<Bits>();
        final List<Integer> starts = new ArrayList<Integer>();

        final int maxDoc = new ReaderUtil.Gather(r) {
          @Override
          protected void add(int base, IndexReader r) throws IOException {
            // record all delDocs, even if they are null
            bits.add(r.getDeletedDocs());
            starts.add(base);
          }
        }.run();
        starts.add(maxDoc);

        assert bits.size() > 0;
        if (bits.size() == 1) {
          // Only one actual sub reader -- optimize this case
          result = bits.get(0);
        } else {
          result = new MultiBits(bits, starts);
        }
        r.storeDelDocs(result);
      }
    } else {
      result = null;
    }

    return result;
  }


, Instance #
frags: 
(startLine=272 endLine=298 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      ByteParser parser = (ByteParser) entry.custom;
      if (parser == null) {
        return wrapper.getBytes(reader, field, FieldCache.DEFAULT_BYTE_PARSER);
      }
      final byte[] retArray = new byte[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          byte termval = parser.parseByte(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }

(startLine=319 endLine=345 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry =  entryKey;
      String field = entry.field;
      ShortParser parser = (ShortParser) entry.custom;
      if (parser == null) {
        return wrapper.getShorts(reader, field, FieldCache.DEFAULT_SHORT_PARSER);
      }
      final short[] retArray = new short[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          short termval = parser.parseShort(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }

commonMethod: 
(startLine=102 endLine=110 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/TermsEnum.java)
  /** Get {@link DocsEnum} for the current term.  Do not
   *  call this before calling {@link #next} or {@link
   *  #seek} for the first time.  This method will not
   *  return null.
   *  
   * @param skipDocs set bits are documents that should not
   * be returned
   * @param reuse pass a prior DocsEnum for possible reuse */
  public abstract DocsEnum docs(Bits skipDocs, DocsEnum reuse) throws IOException;


, Instance #
frags: 
(startLine=272 endLine=298 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      ByteParser parser = (ByteParser) entry.custom;
      if (parser == null) {
        return wrapper.getBytes(reader, field, FieldCache.DEFAULT_BYTE_PARSER);
      }
      final byte[] retArray = new byte[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          byte termval = parser.parseByte(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }

(startLine=319 endLine=345 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry =  entryKey;
      String field = entry.field;
      ShortParser parser = (ShortParser) entry.custom;
      if (parser == null) {
        return wrapper.getShorts(reader, field, FieldCache.DEFAULT_SHORT_PARSER);
      }
      final short[] retArray = new short[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          short termval = parser.parseShort(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }

commonMethod: 
(startLine=135 endLine=143 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  /**  This method may return null if the field does not exist.*/
  public static Terms getTerms(IndexReader r, String field) throws IOException {
    final Fields fields = getFields(r);
    if (fields == null) {
      return null;
    } else {
      return fields.terms(field);
    }
  }


, Instance #
frags: 
(startLine=272 endLine=298 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      ByteParser parser = (ByteParser) entry.custom;
      if (parser == null) {
        return wrapper.getBytes(reader, field, FieldCache.DEFAULT_BYTE_PARSER);
      }
      final byte[] retArray = new byte[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          byte termval = parser.parseByte(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }

(startLine=319 endLine=345 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry =  entryKey;
      String field = entry.field;
      ShortParser parser = (ShortParser) entry.custom;
      if (parser == null) {
        return wrapper.getShorts(reader, field, FieldCache.DEFAULT_SHORT_PARSER);
      }
      final short[] retArray = new short[reader.maxDoc()];
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          short termval = parser.parseShort(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      return retArray;
    }

commonMethod: 
(startLine=99 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  public static Bits getDeletedDocs(IndexReader r) throws IOException {
    Bits result;
    if (r.hasDeletions()) {

      result = r.retrieveDelDocs();
      if (result == null) {

        final List<Bits> bits = new ArrayList<Bits>();
        final List<Integer> starts = new ArrayList<Integer>();

        final int maxDoc = new ReaderUtil.Gather(r) {
          @Override
          protected void add(int base, IndexReader r) throws IOException {
            // record all delDocs, even if they are null
            bits.add(r.getDeletedDocs());
            starts.add(base);
          }
        }.run();
        starts.add(maxDoc);

        assert bits.size() > 0;
        if (bits.size() == 1) {
          // Only one actual sub reader -- optimize this case
          result = bits.get(0);
        } else {
          result = new MultiBits(bits, starts);
        }
        r.storeDelDocs(result);
      }
    } else {
      result = null;
    }

    return result;
  }


, Instance #
frags: 
(startLine=366 endLine=400 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      IntParser parser = (IntParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getInts(reader, field, DEFAULT_INT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getInts(reader, field, NUMERIC_UTILS_INT_PARSER);      
        }
      }
      int[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          if (retArray == null) // late init
            retArray = new int[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new int[reader.maxDoc()];
      return retArray;
    }

(startLine=424 endLine=458 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FloatParser parser = (FloatParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getFloats(reader, field, DEFAULT_FLOAT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getFloats(reader, field, NUMERIC_UTILS_FLOAT_PARSER);      
        }
    }
      float[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          if (retArray == null) // late init
            retArray = new float[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new float[reader.maxDoc()];
      return retArray;
    }

(startLine=479 endLine=512 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      String field = entry.field;
      FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getLongs(reader, field, DEFAULT_LONG_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getLongs(reader, field, NUMERIC_UTILS_LONG_PARSER);      
        }
      }
      long[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term(field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          if (retArray == null) // late init
            retArray = new long[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new long[reader.maxDoc()];
      return retArray;
    }

(startLine=534 endLine=568 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getDoubles(reader, field, DEFAULT_DOUBLE_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getDoubles(reader, field, NUMERIC_UTILS_DOUBLE_PARSER);      
        }
      }
      double[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          if (retArray == null) // late init
            retArray = new double[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new double[reader.maxDoc()];
      return retArray;
    }

commonMethod: 
(startLine=135 endLine=143 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  /**  This method may return null if the field does not exist.*/
  public static Terms getTerms(IndexReader r, String field) throws IOException {
    final Fields fields = getFields(r);
    if (fields == null) {
      return null;
    } else {
      return fields.terms(field);
    }
  }


, Instance #
frags: 
(startLine=366 endLine=400 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      IntParser parser = (IntParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getInts(reader, field, DEFAULT_INT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getInts(reader, field, NUMERIC_UTILS_INT_PARSER);      
        }
      }
      int[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          if (retArray == null) // late init
            retArray = new int[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new int[reader.maxDoc()];
      return retArray;
    }

(startLine=424 endLine=458 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FloatParser parser = (FloatParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getFloats(reader, field, DEFAULT_FLOAT_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getFloats(reader, field, NUMERIC_UTILS_FLOAT_PARSER);      
        }
    }
      float[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          if (retArray == null) // late init
            retArray = new float[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new float[reader.maxDoc()];
      return retArray;
    }

(startLine=479 endLine=512 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      String field = entry.field;
      FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getLongs(reader, field, DEFAULT_LONG_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getLongs(reader, field, NUMERIC_UTILS_LONG_PARSER);      
        }
      }
      long[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term(field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          if (retArray == null) // late init
            retArray = new long[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new long[reader.maxDoc()];
      return retArray;
    }

(startLine=534 endLine=568 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        throws IOException {
      Entry entry = entryKey;
      String field = entry.field;
      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
      if (parser == null) {
        try {
          return wrapper.getDoubles(reader, field, DEFAULT_DOUBLE_PARSER);
        } catch (NumberFormatException ne) {
          return wrapper.getDoubles(reader, field, NUMERIC_UTILS_DOUBLE_PARSER);      
        }
      }
      double[] retArray = null;
      TermDocs termDocs = reader.termDocs();
      TermEnum termEnum = reader.terms (new Term (field));
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          if (retArray == null) // late init
            retArray = new double[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {
      } finally {
        termDocs.close();
        termEnum.close();
      }
      if (retArray == null) // no values
        retArray = new double[reader.maxDoc()];
      return retArray;
    }

commonMethod: 
(startLine=99 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  public static Bits getDeletedDocs(IndexReader r) throws IOException {
    Bits result;
    if (r.hasDeletions()) {

      result = r.retrieveDelDocs();
      if (result == null) {

        final List<Bits> bits = new ArrayList<Bits>();
        final List<Integer> starts = new ArrayList<Integer>();

        final int maxDoc = new ReaderUtil.Gather(r) {
          @Override
          protected void add(int base, IndexReader r) throws IOException {
            // record all delDocs, even if they are null
            bits.add(r.getDeletedDocs());
            starts.add(base);
          }
        }.run();
        starts.add(maxDoc);

        assert bits.size() > 0;
        if (bits.size() == 1) {
          // Only one actual sub reader -- optimize this case
          result = bits.get(0);
        } else {
          result = new MultiBits(bits, starts);
        }
        r.storeDelDocs(result);
      }
    } else {
      result = null;
    }

    return result;
  }


, Instance #
frags: 
(startLine=380 endLine=392 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          if (retArray == null) // late init
            retArray = new int[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=438 endLine=450 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          if (retArray == null) // late init
            retArray = new float[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=492 endLine=504 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          if (retArray == null) // late init
            retArray = new long[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=548 endLine=560 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          if (retArray == null) // late init
            retArray = new double[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

commonMethod: 
(startLine=135 endLine=143 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  /**  This method may return null if the field does not exist.*/
  public static Terms getTerms(IndexReader r, String field) throws IOException {
    final Fields fields = getFields(r);
    if (fields == null) {
      return null;
    } else {
      return fields.terms(field);
    }
  }


, Instance #
frags: 
(startLine=380 endLine=392 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          int termval = parser.parseInt(term.text());
          if (retArray == null) // late init
            retArray = new int[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=438 endLine=450 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          float termval = parser.parseFloat(term.text());
          if (retArray == null) // late init
            retArray = new float[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=492 endLine=504 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          long termval = parser.parseLong(term.text());
          if (retArray == null) // late init
            retArray = new long[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=548 endLine=560 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          double termval = parser.parseDouble(term.text());
          if (retArray == null) // late init
            retArray = new double[reader.maxDoc()];
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

commonMethod: 
(startLine=99 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  public static Bits getDeletedDocs(IndexReader r) throws IOException {
    Bits result;
    if (r.hasDeletions()) {

      result = r.retrieveDelDocs();
      if (result == null) {

        final List<Bits> bits = new ArrayList<Bits>();
        final List<Integer> starts = new ArrayList<Integer>();

        final int maxDoc = new ReaderUtil.Gather(r) {
          @Override
          protected void add(int base, IndexReader r) throws IOException {
            // record all delDocs, even if they are null
            bits.add(r.getDeletedDocs());
            starts.add(base);
          }
        }.run();
        starts.add(maxDoc);

        assert bits.size() > 0;
        if (bits.size() == 1) {
          // Only one actual sub reader -- optimize this case
          result = bits.get(0);
        } else {
          result = new MultiBits(bits, starts);
        }
        r.storeDelDocs(result);
      }
    } else {
      result = null;
    }

    return result;
  }


, Instance #
frags: 
(startLine=282 endLine=292 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          byte termval = parser.parseByte(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=329 endLine=339 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          short termval = parser.parseShort(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=589 endLine=599 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          String termval = term.text();
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {

(startLine=634 endLine=649 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;

          // store term text
          mterms[t] = term.text();

          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = t;
          }

          t++;
        } while (termEnum.next());
      } finally {

commonMethod: 
(startLine=135 endLine=143 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  /**  This method may return null if the field does not exist.*/
  public static Terms getTerms(IndexReader r, String field) throws IOException {
    final Fields fields = getFields(r);
    if (fields == null) {
      return null;
    } else {
      return fields.terms(field);
    }
  }


, Instance #
frags: 
(startLine=282 endLine=292 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          byte termval = parser.parseByte(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=329 endLine=339 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          short termval = parser.parseShort(term.text());
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } catch (StopFillCacheException stop) {

(startLine=589 endLine=599 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;
          String termval = term.text();
          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = termval;
          }
        } while (termEnum.next());
      } finally {

(startLine=634 endLine=649 srcPath=/home/sonia/NewExperiment/luceneFilter/00667/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      try {
        do {
          Term term = termEnum.term();
          if (term==null || term.field() != field) break;

          // store term text
          mterms[t] = term.text();

          termDocs.seek (termEnum);
          while (termDocs.next()) {
            retArray[termDocs.doc()] = t;
          }

          t++;
        } while (termEnum.next());
      } finally {

commonMethod: 
(startLine=99 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00668/lucene/src/java/org/apache/lucene/index/MultiFields.java)
  public static Bits getDeletedDocs(IndexReader r) throws IOException {
    Bits result;
    if (r.hasDeletions()) {

      result = r.retrieveDelDocs();
      if (result == null) {

        final List<Bits> bits = new ArrayList<Bits>();
        final List<Integer> starts = new ArrayList<Integer>();

        final int maxDoc = new ReaderUtil.Gather(r) {
          @Override
          protected void add(int base, IndexReader r) throws IOException {
            // record all delDocs, even if they are null
            bits.add(r.getDeletedDocs());
            starts.add(base);
          }
        }.run();
        starts.add(maxDoc);

        assert bits.size() > 0;
        if (bits.size() == 1) {
          // Only one actual sub reader -- optimize this case
          result = bits.get(0);
        } else {
          result = new MultiBits(bits, starts);
        }
        r.storeDelDocs(result);
      }
    } else {
      result = null;
    }

    return result;
  }


, Instance #
frags: 
(startLine=712 endLine=737 srcPath=/home/sonia/NewExperiment/luceneFilter/00737/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      if (terms != null) {
        final TermsEnum termsEnum = terms.iterator();
        final Bits delDocs = MultiFields.getDeletedDocs(reader);
        DocsEnum docs = null;
        while(true) {
          final BytesRef term = termsEnum.next();
          if (term == null) {
            break;
          }

          // store term text
          mterms[t] = term.utf8ToString();
          //System.out.println("FC:  ord=" + t + " term=" + term.toBytesString());

          docs = termsEnum.docs(delDocs, docs);
          while (true) {
            final int docID = docs.nextDoc();
            if (docID == DocsEnum.NO_MORE_DOCS) {
              break;
            }
            //System.out.println("FC:    docID=" + docID);
            retArray[docID] = t;
          }
          t++;
        }
      }

(startLine=659 endLine=678 srcPath=/home/sonia/NewExperiment/luceneFilter/00737/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
      if (terms != null) {
        final TermsEnum termsEnum = terms.iterator();
        final Bits delDocs = MultiFields.getDeletedDocs(reader);
        DocsEnum docs = null;
        while(true) {
          final BytesRef term = termsEnum.next();
          if (term == null) {
            break;
          }
          docs = termsEnum.docs(delDocs, docs);
          final String termval = term.utf8ToString();
          while (true) {
            final int docID = docs.nextDoc();
            if (docID == DocsEnum.NO_MORE_DOCS) {
              break;
            }
            retArray[docID] = termval;
          }
        }
      }

commonMethod: 
(startLine=63 endLine=79 srcPath=/home/sonia/NewExperiment/luceneFilter/00738/lucene/src/java/org/apache/lucene/util/packed/GrowableWriter.java)
  public void set(int index, long value) {
    if (value >= currentMaxValue) {
      int bpv = getBitsPerValue();
      while(currentMaxValue <= value && currentMaxValue != Long.MAX_VALUE) {
        bpv++;
        currentMaxValue *= 2;
      }
      final int valueCount = size();
      PackedInts.Mutable next = PackedInts.getMutable(valueCount, getSize(bpv));
      for(int i=0;i<valueCount;i++) {
        next.set(i, current.get(i));
      }
      current = next;
      currentMaxValue = PackedInts.maxValue(current.getBitsPerValue());
    }
    current.set(index, value);
  }


, Instance #
frags: 
(startLine=716 endLine=736 srcPath=/home/sonia/NewExperiment/luceneFilter/00737/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        while(true) {
          final BytesRef term = termsEnum.next();
          if (term == null) {
            break;
          }

          // store term text
          mterms[t] = term.utf8ToString();
          //System.out.println("FC:  ord=" + t + " term=" + term.toBytesString());

          docs = termsEnum.docs(delDocs, docs);
          while (true) {
            final int docID = docs.nextDoc();
            if (docID == DocsEnum.NO_MORE_DOCS) {
              break;
            }
            //System.out.println("FC:    docID=" + docID);
            retArray[docID] = t;
          }
          t++;
        }

(startLine=663 endLine=677 srcPath=/home/sonia/NewExperiment/luceneFilter/00737/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java)
        while(true) {
          final BytesRef term = termsEnum.next();
          if (term == null) {
            break;
          }
          docs = termsEnum.docs(delDocs, docs);
          final String termval = term.utf8ToString();
          while (true) {
            final int docID = docs.nextDoc();
            if (docID == DocsEnum.NO_MORE_DOCS) {
              break;
            }
            retArray[docID] = termval;
          }
        }

commonMethod: 
(startLine=63 endLine=79 srcPath=/home/sonia/NewExperiment/luceneFilter/00738/lucene/src/java/org/apache/lucene/util/packed/GrowableWriter.java)
  public void set(int index, long value) {
    if (value >= currentMaxValue) {
      int bpv = getBitsPerValue();
      while(currentMaxValue <= value && currentMaxValue != Long.MAX_VALUE) {
        bpv++;
        currentMaxValue *= 2;
      }
      final int valueCount = size();
      PackedInts.Mutable next = PackedInts.getMutable(valueCount, getSize(bpv));
      for(int i=0;i<valueCount;i++) {
        next.set(i, current.get(i));
      }
      current = next;
      currentMaxValue = PackedInts.maxValue(current.getBitsPerValue());
    }
    current.set(index, value);
  }


, Instance #
frags: 
(startLine=614 endLine=652 srcPath=/home/sonia/NewExperiment/luceneFilter/00754/lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java)
  public void testFarsi() throws Exception {

    /* build an index */
    RAMDirectory farsiIndex = new RAMDirectory();
    IndexWriter writer = new IndexWriter(farsiIndex, new IndexWriterConfig(
        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true)));
    Document doc = new Document();
    doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
        Field.Index.NOT_ANALYZED));
    doc
        .add(new Field("body", "body", Field.Store.YES,
            Field.Index.NOT_ANALYZED));
    writer.addDocument(doc);

    writer.optimize();
    writer.close();

    IndexReader reader = IndexReader.open(farsiIndex, true);
    IndexSearcher search = new IndexSearcher(reader);

    // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
    // RuleBasedCollator. However, the Arabic Locale seems to order the Farsi
    // characters properly.
    Collator c = Collator.getInstance(new Locale("ar"));

    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
    // orders the U+0698 character before the U+0633 character, so the single
    // index Term below should NOT be returned by a ConstantScoreRangeQuery
    // with a Farsi Collator (or an Arabic one for the case when Farsi is
    // not supported).
    ScoreDoc[] result = search.search(csrq("content", "\u062F", "\u0698", T, T,
        c), null, 1000).scoreDocs;
    assertEquals("The index Term should not be included.", 0, result.length);

    result = search.search(csrq("content", "\u0633", "\u0638", T, T, c), null,
        1000).scoreDocs;
    assertEquals("The index Term should be included.", 1, result.length);
    search.close();
  }

(startLine=654 endLine=690 srcPath=/home/sonia/NewExperiment/luceneFilter/00754/lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java)
  public void testDanish() throws Exception {

    /* build an index */
    RAMDirectory danishIndex = new RAMDirectory();
    IndexWriter writer = new IndexWriter(danishIndex, new IndexWriterConfig(
        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true)));

    // Danish collation orders the words below in the given order
    // (example taken from TestSort.testInternationalSort() ).
    String[] words = { "H\u00D8T", "H\u00C5T", "MAND" };
    for (int docnum = 0 ; docnum < words.length ; ++docnum) {   
      Document doc = new Document();
      doc.add(new Field("content", words[docnum], 
                        Field.Store.YES, Field.Index.NOT_ANALYZED));
      doc.add(new Field("body", "body",
                        Field.Store.YES, Field.Index.NOT_ANALYZED));
      writer.addDocument(doc);
    }
    writer.optimize();
    writer.close();

    IndexReader reader = IndexReader.open(danishIndex, true);
    IndexSearcher search = new IndexSearcher(reader);

    Collator c = Collator.getInstance(new Locale("da", "dk"));

    // Unicode order would not include "H\u00C5T" in [ "H\u00D8T", "MAND" ],
    // but Danish collation does.
    ScoreDoc[] result = search.search
      (csrq("content", "H\u00D8T", "MAND", F, F, c), null, 1000).scoreDocs;
    assertEquals("The index Term should be included.", 1, result.length);

    result = search.search
      (csrq("content", "H\u00C5T", "MAND", F, F, c), null, 1000).scoreDocs;
    assertEquals("The index Term should not be included.", 0, result.length);
    search.close();
  }

commonMethod: 
(startLine=96 endLine=103 srcPath=/home/sonia/NewExperiment/luceneFilter/00755/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java)
  public IndexReader getReader() throws IOException {
    if (r.nextBoolean()) {
      return w.getReader();
    } else {
      w.commit();
      return IndexReader.open(w.getDirectory(), new KeepOnlyLastCommitDeletionPolicy(), r.nextBoolean(), _TestUtil.nextInt(r, 1, 10));
    }
  }


, Instance #
frags: 
(startLine=1572 endLine=1598 srcPath=/home/sonia/NewExperiment/luceneFilter/00774/lucene/src/test/org/apache/lucene/index/TestIndexReader.java)
  public void testFieldCacheReuseAfterClone() throws Exception {
    Directory dir = new MockRAMDirectory();
    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
    Document doc = new Document();
    doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
    writer.addDocument(doc);
    writer.close();

    // Open reader
    IndexReader r = SegmentReader.getOnlySegmentReader(dir);
    final int[] ints = FieldCache.DEFAULT.getInts(r, "number");
    assertEquals(1, ints.length);
    assertEquals(17, ints[0]);

    // Clone reader
    IndexReader r2 = (IndexReader) r.clone();
    r.close();
    assertTrue(r2 != r);
    final int[] ints2 = FieldCache.DEFAULT.getInts(r2, "number");
    r2.close();

    assertEquals(1, ints2.length);
    assertEquals(17, ints2[0]);
    assertTrue(ints == ints2);

    dir.close();
  }

(startLine=1603 endLine=1631 srcPath=/home/sonia/NewExperiment/luceneFilter/00774/lucene/src/test/org/apache/lucene/index/TestIndexReader.java)
  public void testFieldCacheReuseAfterReopen() throws Exception {
    Directory dir = new MockRAMDirectory();
    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
    Document doc = new Document();
    doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
    writer.addDocument(doc);
    writer.commit();

    // Open reader1
    IndexReader r = IndexReader.open(dir, false);
    IndexReader r1 = SegmentReader.getOnlySegmentReader(r);
    final int[] ints = FieldCache.DEFAULT.getInts(r1, "number");
    assertEquals(1, ints.length);
    assertEquals(17, ints[0]);

    // Add new segment
    writer.addDocument(doc);
    writer.commit();

    // Reopen reader1 --> reader2
    IndexReader r2 = r.reopen();
    r.close();
    IndexReader sub0 = r2.getSequentialSubReaders()[0];
    final int[] ints2 = FieldCache.DEFAULT.getInts(sub0, "number");
    r2.close();
    assertTrue(ints == ints2);

    dir.close();
  }

commonMethod: 
(startLine=311 endLine=314 srcPath=/home/sonia/NewExperiment/luceneFilter/00775/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java)
  /** create a new index writer config with random defaults */
  public static IndexWriterConfig newIndexWriterConfig(Random r, Version v, Analyzer a) {
    return LuceneTestCaseJ4.newIndexWriterConfig(r, v, a);
  }


, Instance #
frags: 
(startLine=1814 endLine=1895 srcPath=/home/sonia/NewExperiment/luceneFilter/00774/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java)
  public void testDocumentsWriterExceptions() throws IOException {
    Analyzer analyzer = new Analyzer() {
      @Override
      public TokenStream tokenStream(String fieldName, Reader reader) {
        return new CrashingFilter(fieldName, new MockTokenizer(reader, MockTokenizer.WHITESPACE, false));
      }
    };

    for(int i=0;i<2;i++) {
      MockRAMDirectory dir = new MockRAMDirectory();
      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
      //writer.setInfoStream(System.out);
      Document doc = new Document();
      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
                        Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
      writer.addDocument(doc);
      writer.addDocument(doc);
      doc.add(new Field("crash", "this should crash after 4 terms", Field.Store.YES,
                        Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
      doc.add(new Field("other", "this will not get indexed", Field.Store.YES,
                        Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
      try {
        writer.addDocument(doc);
        fail("did not hit expected exception");
      } catch (IOException ioe) {
      }

      if (0 == i) {
        doc = new Document();
        doc.add(new Field("contents", "here are some contents", Field.Store.YES,
                          Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
        writer.addDocument(doc);
        writer.addDocument(doc);
      }
      writer.close();

      IndexReader reader = IndexReader.open(dir, true);
      int expected = 3+(1-i)*2;
      assertEquals(expected, reader.docFreq(new Term("contents", "here")));
      assertEquals(expected, reader.maxDoc());
      int numDel = 0;
      for(int j=0;j<reader.maxDoc();j++) {
        if (reader.isDeleted(j))
          numDel++;
        else {
          reader.document(j);
          reader.getTermFreqVectors(j);
        }
      }
      reader.close();

      assertEquals(1, numDel);

      writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,
          analyzer).setMaxBufferedDocs(10));
      doc = new Document();
      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
                        Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
      for(int j=0;j<17;j++)
        writer.addDocument(doc);
      writer.optimize();
      writer.close();

      reader = IndexReader.open(dir, true);
      expected = 19+(1-i)*2;
      assertEquals(expected, reader.docFreq(new Term("contents", "here")));
      assertEquals(expected, reader.maxDoc());
      numDel = 0;
      for(int j=0;j<reader.maxDoc();j++) {
        if (reader.isDeleted(j))
          numDel++;
        else {
          reader.document(j);
          reader.getTermFreqVectors(j);
        }
      }
      reader.close();
      assertEquals(0, numDel);

      dir.close();
    }
  }

(startLine=1897 endLine=2009 srcPath=/home/sonia/NewExperiment/luceneFilter/00774/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java)
  public void testDocumentsWriterExceptionThreads() throws Exception {
    Analyzer analyzer = new Analyzer() {
      @Override
      public TokenStream tokenStream(String fieldName, Reader reader) {
        return new CrashingFilter(fieldName, new MockTokenizer(reader, MockTokenizer.WHITESPACE, false));
      }
    };

    final int NUM_THREAD = 3;
    final int NUM_ITER = 100;

    for(int i=0;i<2;i++) {
      MockRAMDirectory dir = new MockRAMDirectory();

      {
        final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));

        final int finalI = i;

        Thread[] threads = new Thread[NUM_THREAD];
        for(int t=0;t<NUM_THREAD;t++) {
          threads[t] = new Thread() {
              @Override
              public void run() {
                try {
                  for(int iter=0;iter<NUM_ITER;iter++) {
                    Document doc = new Document();
                    doc.add(new Field("contents", "here are some contents", Field.Store.YES,
                                      Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                    writer.addDocument(doc);
                    writer.addDocument(doc);
                    doc.add(new Field("crash", "this should crash after 4 terms", Field.Store.YES,
                                      Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                    doc.add(new Field("other", "this will not get indexed", Field.Store.YES,
                                      Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                    try {
                      writer.addDocument(doc);
                      fail("did not hit expected exception");
                    } catch (IOException ioe) {
                    }

                    if (0 == finalI) {
                      doc = new Document();
                      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
                                        Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                      writer.addDocument(doc);
                      writer.addDocument(doc);
                    }
                  }
                } catch (Throwable t) {
                  synchronized(this) {
                    System.out.println(Thread.currentThread().getName() + ": ERROR: hit unexpected exception");
                    t.printStackTrace(System.out);
                  }
                  fail();
                }
              }
            };
          threads[t].start();
        }

        for(int t=0;t<NUM_THREAD;t++)
          threads[t].join();
            
        writer.close();
      }

      IndexReader reader = IndexReader.open(dir, true);
      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;
      assertEquals(expected, reader.docFreq(new Term("contents", "here")));
      assertEquals(expected, reader.maxDoc());
      int numDel = 0;
      for(int j=0;j<reader.maxDoc();j++) {
        if (reader.isDeleted(j))
          numDel++;
        else {
          reader.document(j);
          reader.getTermFreqVectors(j);
        }
      }
      reader.close();

      assertEquals(NUM_THREAD*NUM_ITER, numDel);

      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));
      Document doc = new Document();
      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
                        Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
      for(int j=0;j<17;j++)
        writer.addDocument(doc);
      writer.optimize();
      writer.close();

      reader = IndexReader.open(dir, true);
      expected += 17-NUM_THREAD*NUM_ITER;
      assertEquals(expected, reader.docFreq(new Term("contents", "here")));
      assertEquals(expected, reader.maxDoc());
      numDel = 0;
      for(int j=0;j<reader.maxDoc();j++) {
        if (reader.isDeleted(j))
          numDel++;
        else {
          reader.document(j);
          reader.getTermFreqVectors(j);
        }
      }
      reader.close();
      assertEquals(0, numDel);

      dir.close();
    }
  }

commonMethod: 
(startLine=311 endLine=314 srcPath=/home/sonia/NewExperiment/luceneFilter/00775/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java)
  /** create a new index writer config with random defaults */
  public static IndexWriterConfig newIndexWriterConfig(Random r, Version v, Analyzer a) {
    return LuceneTestCaseJ4.newIndexWriterConfig(r, v, a);
  }


, Instance #
frags: 
(startLine=235 endLine=311 srcPath=/home/sonia/NewExperiment/luceneFilter/00792/lucene/src/java/org/apache/lucene/search/MultiPhraseQuery.java)
      throws IOException {
      ComplexExplanation result = new ComplexExplanation();
      result.setDescription("weight("+getQuery()+" in "+doc+"), product of:");

      Explanation idfExpl = new Explanation(idf, "idf("+getQuery()+")");

      // explain query weight
      Explanation queryExpl = new Explanation();
      queryExpl.setDescription("queryWeight(" + getQuery() + "), product of:");

      Explanation boostExpl = new Explanation(getBoost(), "boost");
      if (getBoost() != 1.0f)
        queryExpl.addDetail(boostExpl);

      queryExpl.addDetail(idfExpl);

      Explanation queryNormExpl = new Explanation(queryNorm,"queryNorm");
      queryExpl.addDetail(queryNormExpl);

      queryExpl.setValue(boostExpl.getValue() *
                         idfExpl.getValue() *
                         queryNormExpl.getValue());

      result.addDetail(queryExpl);

      // explain field weight
      ComplexExplanation fieldExpl = new ComplexExplanation();
      fieldExpl.setDescription("fieldWeight("+getQuery()+" in "+doc+
                               "), product of:");

      Scorer scorer = (Scorer) scorer(reader, true, false);
      if (scorer == null) {
        return new Explanation(0.0f, "no matching docs");
      }

      Explanation tfExplanation = new Explanation();
      int d = scorer.advance(doc);
      float phraseFreq;
      if (d == doc) {
        if (slop == 0) {
          phraseFreq = ((ExactPhraseScorer) scorer).currentFreq();
        } else {
          phraseFreq = ((SloppyPhraseScorer) scorer).currentFreq();
        }
      } else {
        phraseFreq = 0.0f;
      }

      tfExplanation.setValue(similarity.tf(phraseFreq));
      tfExplanation.setDescription("tf(phraseFreq=" + phraseFreq + ")");
      fieldExpl.addDetail(tfExplanation);
      fieldExpl.addDetail(idfExpl);

      Explanation fieldNormExpl = new Explanation();
      byte[] fieldNorms = reader.norms(field);
      float fieldNorm =
        fieldNorms!=null ? similarity.decodeNormValue(fieldNorms[doc]) : 1.0f;
      fieldNormExpl.setValue(fieldNorm);
      fieldNormExpl.setDescription("fieldNorm(field="+field+", doc="+doc+")");
      fieldExpl.addDetail(fieldNormExpl);

      fieldExpl.setMatch(Boolean.valueOf(tfExplanation.isMatch()));
      fieldExpl.setValue(tfExplanation.getValue() *
                         idfExpl.getValue() *
                         fieldNormExpl.getValue());

      result.addDetail(fieldExpl);
      result.setMatch(fieldExpl.getMatch());

      // combine them
      result.setValue(queryExpl.getValue() * fieldExpl.getValue());

      if (queryExpl.getValue() == 1.0f)
        return fieldExpl;

      return result;
    }

(startLine=224 endLine=314 srcPath=/home/sonia/NewExperiment/luceneFilter/00792/lucene/src/java/org/apache/lucene/search/PhraseQuery.java)
      throws IOException {

      Explanation result = new Explanation();
      result.setDescription("weight("+getQuery()+" in "+doc+"), product of:");

      StringBuilder docFreqs = new StringBuilder();
      StringBuilder query = new StringBuilder();
      query.append('\"');
      docFreqs.append(idfExp.explain());
      for (int i = 0; i < terms.size(); i++) {
        if (i != 0) {
          query.append(" ");
        }

        Term term = terms.get(i);

        query.append(term.text());
      }
      query.append('\"');

      Explanation idfExpl =
        new Explanation(idf, "idf(" + field + ":" + docFreqs + ")");

      // explain query weight
      Explanation queryExpl = new Explanation();
      queryExpl.setDescription("queryWeight(" + getQuery() + "), product of:");

      Explanation boostExpl = new Explanation(getBoost(), "boost");
      if (getBoost() != 1.0f)
        queryExpl.addDetail(boostExpl);
      queryExpl.addDetail(idfExpl);

      Explanation queryNormExpl = new Explanation(queryNorm,"queryNorm");
      queryExpl.addDetail(queryNormExpl);

      queryExpl.setValue(boostExpl.getValue() *
                         idfExpl.getValue() *
                         queryNormExpl.getValue());

      result.addDetail(queryExpl);

      // explain field weight
      Explanation fieldExpl = new Explanation();
      fieldExpl.setDescription("fieldWeight("+field+":"+query+" in "+doc+
                               "), product of:");

      Scorer scorer = (Scorer) scorer(reader, true, false);
      if (scorer == null) {
        return new Explanation(0.0f, "no matching docs");
      }
      Explanation tfExplanation = new Explanation();
      int d = scorer.advance(doc);
      float phraseFreq;
      if (d == doc) {
        if (slop == 0) {
          phraseFreq = ((ExactPhraseScorer) scorer).currentFreq();
        } else {
          phraseFreq = ((SloppyPhraseScorer) scorer).currentFreq();
        }
      } else {
        phraseFreq = 0.0f;
      }

      tfExplanation.setValue(similarity.tf(phraseFreq));
      tfExplanation.setDescription("tf(phraseFreq=" + phraseFreq + ")");
      
      fieldExpl.addDetail(tfExplanation);
      fieldExpl.addDetail(idfExpl);

      Explanation fieldNormExpl = new Explanation();
      byte[] fieldNorms = reader.norms(field);
      float fieldNorm =
        fieldNorms!=null ? similarity.decodeNormValue(fieldNorms[doc]) : 1.0f;
      fieldNormExpl.setValue(fieldNorm);
      fieldNormExpl.setDescription("fieldNorm(field="+field+", doc="+doc+")");
      fieldExpl.addDetail(fieldNormExpl);

      fieldExpl.setValue(tfExplanation.getValue() *
                         idfExpl.getValue() *
                         fieldNormExpl.getValue());

      result.addDetail(fieldExpl);

      // combine them
      result.setValue(queryExpl.getValue() * fieldExpl.getValue());

      if (queryExpl.getValue() == 1.0f)
        return fieldExpl;

      return result;
    }

commonMethod: 
(startLine=110 endLine=118 srcPath=/home/sonia/NewExperiment/luceneFilter/00793/lucene/src/java/org/apache/lucene/search/Scorer.java)
  /** Returns number of matches for the current document.
   *  This returns a float (not int) because
   *  SloppyPhraseScorer discounts its freq according to how
   *  "sloppy" the match was.
   *
   * @lucene.experimental */
  public float freq() throws IOException {
    throw new UnsupportedOperationException(this + " does not implement freq()");
  }


, Instance #
frags: 
(startLine=497 endLine=504 srcPath=/home/sonia/NewExperiment/luceneFilter/00800/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java)
              if (updates) {
                Document d = new Document();
                d.add(new Field("id", Integer.toString(i), Field.Store.YES,
                                Field.Index.NOT_ANALYZED));
                d.add(new Field("content", "bbb " + i, Field.Store.NO,
                                Field.Index.ANALYZED));
                modifier.updateDocument(new Term("id", Integer.toString(docId)), d);
              } else { // deletes

(startLine=72 endLine=77 srcPath=/home/sonia/NewExperiment/luceneFilter/00800/lucene/src/test/org/apache/lucene/search/TestExplanations.java)
    for (int i = 0; i < docFields.length; i++) {
      Document doc = new Document();
      doc.add(new Field(KEY, ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
      doc.add(new Field(FIELD, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
      writer.addDocument(doc);
    }

commonMethod: 
(startLine=350 endLine=352 srcPath=/home/sonia/NewExperiment/luceneFilter/00801/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java)
  public Field newField(String name, String value, Store store, Index index) {
    return LuceneTestCaseJ4.newField(random, name, value, store, index);
  }


, Instance #
frags: 
(startLine=72 endLine=105 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testNumDocsLimit() throws Exception {
    // tests that the max merge docs constraint is applied during optimize.
    Directory dir = new RAMDirectory();

    // Prepare an index w/ several small segments and a large one.
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    // prevent any merges from happening.
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);

    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 5);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    
    writer.close();

    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();

    // Should only be 3 segments in the index, because one of them exceeds the size limit
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(3, sis.size());
  }

(startLine=107 endLine=133 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testLastSegmentTooLarge() throws Exception {
    Directory dir = new RAMDirectory();

    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);

    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 5);
    
    writer.close();

    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();

    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(2, sis.size());
  }

(startLine=135 endLine=161 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testFirstSegmentTooLarge() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 5);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    
    writer.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(2, sis.size());
  }

(startLine=163 endLine=189 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testAllSegmentsSmall() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    
    writer.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(1, sis.size());
  }

(startLine=191 endLine=216 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testAllSegmentsLarge() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    
    writer.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(2);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(3, sis.size());
  }

(startLine=218 endLine=244 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testOneLargeOneSmall() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 3);
    addDocs(writer, 5);
    addDocs(writer, 3);
    addDocs(writer, 5);
    
    writer.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(4, sis.size());
  }

(startLine=246 endLine=278 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testMergeFactor() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 3);
    addDocs(writer, 5);
    addDocs(writer, 3);
    addDocs(writer, 3);
    
    writer.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    lmp.setMergeFactor(2);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    // Should only be 4 segments in the index, because of the merge factor and
    // max merge docs settings.
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(4, sis.size());
  }

(startLine=314 endLine=338 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testSingleOptimizedSegment() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 3);
    
    writer.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    // Verify that the last segment does not have deletions.
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(1, sis.size());
  }

commonMethod: 
(startLine=37 endLine=44 srcPath=/home/sonia/NewExperiment/luceneFilter/00838/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  private static IndexWriterConfig newWriterConfig() throws IOException {
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);
    // prevent any merges by default.
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    return conf;
  }


, Instance #
frags: 
(startLine=280 endLine=312 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testSingleNonOptimizedSegment() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 3);
    addDocs(writer, 5);
    addDocs(writer, 3);
    
    writer.close();
  
    // delete the last document, so that the last segment is optimized.
    IndexReader r = IndexReader.open(dir, false);
    r.deleteDocument(r.numDocs() - 1);
    r.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(3);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    // Verify that the last segment does not have deletions.
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(3, sis.size());
    assertFalse(sis.info(2).hasDeletions());
  }

(startLine=340 endLine=370 srcPath=/home/sonia/NewExperiment/luceneFilter/00837/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  public void testSingleNonOptimizedTooLargeSegment() throws Exception {
    Directory dir = new RAMDirectory();
    
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    IndexWriter writer = new IndexWriter(dir, conf);
    
    addDocs(writer, 5);
    
    writer.close();
  
    // delete the last document
    IndexReader r = IndexReader.open(dir, false);
    r.deleteDocument(r.numDocs() - 1);
    r.close();
    
    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    LogMergePolicy lmp = new LogDocMergePolicy();
    lmp.setMaxMergeDocs(2);
    conf.setMergePolicy(lmp);
    
    writer = new IndexWriter(dir, conf);
    writer.optimize();
    writer.close();
    
    // Verify that the last segment does not have deletions.
    SegmentInfos sis = new SegmentInfos();
    sis.read(dir);
    assertEquals(1, sis.size());
    assertTrue(sis.info(0).hasDeletions());
  }

commonMethod: 
(startLine=37 endLine=44 srcPath=/home/sonia/NewExperiment/luceneFilter/00838/lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize.java)
  private static IndexWriterConfig newWriterConfig() throws IOException {
    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);
    // prevent any merges by default.
    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);
    return conf;
  }


, Instance #
frags: 
(startLine=80 endLine=89 srcPath=/home/sonia/NewExperiment/luceneFilter/00856/modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java)
      try {
        DataInputStream in = new DataInputStream(new BufferedInputStream(stream));
        String method = in.readUTF().toUpperCase();
        if (method.indexOf('M') < 0) {
          DEFAULT_TABLE = new org.egothor.stemmer.Trie(in);
        } else {
          DEFAULT_TABLE = new org.egothor.stemmer.MultiTrie2(in);
        }
        in.close();
      } catch (IOException ex) {

(startLine=47 endLine=59 srcPath=/home/sonia/NewExperiment/luceneFilter/00856/modules/analysis/stempel/src/java/org/apache/lucene/analysis/stempel/StempelStemmer.java)
  public StempelStemmer(InputStream stemmerTable) throws IOException {
    if (stemmerTable == null) return;
    
    DataInputStream in = new DataInputStream(new BufferedInputStream(
        stemmerTable));
    String method = in.readUTF().toUpperCase();
    if (method.indexOf('M') < 0) {
      stemmer = new org.egothor.stemmer.Trie(in);
    } else {
      stemmer = new org.egothor.stemmer.MultiTrie2(in);
    }
    in.close();
  }

commonMethod: 
(startLine=61 endLine=77 srcPath=/home/sonia/NewExperiment/luceneFilter/00857/modules/analysis/stempel/src/java/org/apache/lucene/analysis/stempel/StempelStemmer.java)
  /**
   * Load a stemmer table from an inputstream.
   */
  public static Trie load(InputStream stemmerTable) throws IOException {
    DataInputStream in = null;
    try {
      in = new DataInputStream(new BufferedInputStream(stemmerTable));
      String method = in.readUTF().toUpperCase(Locale.ENGLISH);
      if (method.indexOf('M') < 0) {
        return new org.egothor.stemmer.Trie(in);
      } else {
        return new org.egothor.stemmer.MultiTrie2(in);
      }
    } finally {
      in.close();
    }
  }


, Instance #
frags: 
(startLine=48 endLine=59 srcPath=/home/sonia/NewExperiment/luceneFilter/00899/solr/src/solrj/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java)
    for (Map.Entry<String, Object> entry : fieldTypesNL) {
      Analysis analysis = new Analysis();
      NamedList fieldTypeNL = (NamedList) entry.getValue();
      NamedList<Object> queryNL = (NamedList<Object>) fieldTypeNL.get("query");
      List<AnalysisPhase> phases = (queryNL == null) ? null : buildPhases(queryNL);
      analysis.setQueryPhases(phases);
      NamedList<Object> indexNL = (NamedList<Object>) fieldTypeNL.get("index");
      phases = buildPhases(indexNL);
      analysis.setIndexPhases(phases);
      String fieldTypeName = entry.getKey();
      analysisByFieldTypeName.put(fieldTypeName, analysis);
    }

(startLine=62 endLine=73 srcPath=/home/sonia/NewExperiment/luceneFilter/00899/solr/src/solrj/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java)
    for (Map.Entry<String, Object> entry : fieldNamesNL) {
      Analysis analysis = new Analysis();
      NamedList fieldNameNL = (NamedList) entry.getValue();
      NamedList<Object> queryNL = (NamedList<Object>) fieldNameNL.get("query");
      List<AnalysisPhase> phases = (queryNL == null) ? null : buildPhases(queryNL);
      analysis.setQueryPhases(phases);
      NamedList<Object> indexNL = (NamedList<Object>) fieldNameNL.get("index");
      phases = buildPhases(indexNL);
      analysis.setIndexPhases(phases);
      String fieldName = entry.getKey();
      analysisByFieldName.put(fieldName, analysis);
    }

commonMethod: 
(startLine=62 endLine=74 srcPath=/home/sonia/NewExperiment/luceneFilter/00900/solr/src/solrj/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java)
  private Analysis buildAnalysis(NamedList<NamedList<List<NamedList<Object>>>> value) {
      Analysis analysis = new Analysis();
      
      NamedList<List<NamedList<Object>>> queryNL = value.get("query");
      List<AnalysisPhase> phases = (queryNL == null) ? null : buildPhases(queryNL);
      analysis.setQueryPhases(phases);

      NamedList<List<NamedList<Object>>> indexNL = value.get("index");
      phases = buildPhases(indexNL);
      analysis.setIndexPhases(phases);
      
      return analysis;
  }


, Instance #
frags: 
(startLine=185 endLine=201 srcPath=/home/sonia/NewExperiment/luceneFilter/00901/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReaderImpl.java)
  public DocsEnum docs(FieldInfo field, TermState _termState, Bits skipDocs, DocsEnum reuse) throws IOException {
    PulsingTermState termState = (PulsingTermState) _termState;
    if (termState.docFreq <= maxPulsingDocFreq) {
      if (reuse instanceof PulsingDocsEnum) {
        return ((PulsingDocsEnum) reuse).reset(skipDocs, termState);
      } else {
        PulsingDocsEnum docsEnum = new PulsingDocsEnum();
        return docsEnum.reset(skipDocs, termState);
      }
    } else {
      if (reuse instanceof PulsingDocsEnum) {
        return wrappedPostingsReader.docs(field, termState.wrappedTermState, skipDocs, null);
      } else {
        return wrappedPostingsReader.docs(field, termState.wrappedTermState, skipDocs, reuse);
      }
    }
  }

(startLine=205 endLine=221 srcPath=/home/sonia/NewExperiment/luceneFilter/00901/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReaderImpl.java)
  public DocsAndPositionsEnum docsAndPositions(FieldInfo field, TermState _termState, Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException {
    PulsingTermState termState = (PulsingTermState) _termState;
    if (termState.docFreq <= maxPulsingDocFreq) {
      if (reuse instanceof PulsingDocsAndPositionsEnum) {
        return ((PulsingDocsAndPositionsEnum) reuse).reset(skipDocs, termState);
      } else {
        PulsingDocsAndPositionsEnum postingsEnum = new PulsingDocsAndPositionsEnum();
        return postingsEnum.reset(skipDocs, termState);
      }
    } else {
      if (reuse instanceof PulsingDocsAndPositionsEnum) {
        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, skipDocs, null);
      } else {
        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, skipDocs, reuse);
      }
    }
  }

commonMethod: 
(startLine=219 endLine=221 srcPath=/home/sonia/NewExperiment/luceneFilter/00902/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReaderImpl.java)
    boolean canReuse(FieldInfo fieldInfo) {
      return omitTF == fieldInfo.omitTermFreqAndPositions && storePayloads == fieldInfo.storePayloads;
    }


, Instance #
frags: 
(startLine=185 endLine=201 srcPath=/home/sonia/NewExperiment/luceneFilter/00901/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReaderImpl.java)
  public DocsEnum docs(FieldInfo field, TermState _termState, Bits skipDocs, DocsEnum reuse) throws IOException {
    PulsingTermState termState = (PulsingTermState) _termState;
    if (termState.docFreq <= maxPulsingDocFreq) {
      if (reuse instanceof PulsingDocsEnum) {
        return ((PulsingDocsEnum) reuse).reset(skipDocs, termState);
      } else {
        PulsingDocsEnum docsEnum = new PulsingDocsEnum();
        return docsEnum.reset(skipDocs, termState);
      }
    } else {
      if (reuse instanceof PulsingDocsEnum) {
        return wrappedPostingsReader.docs(field, termState.wrappedTermState, skipDocs, null);
      } else {
        return wrappedPostingsReader.docs(field, termState.wrappedTermState, skipDocs, reuse);
      }
    }
  }

(startLine=205 endLine=221 srcPath=/home/sonia/NewExperiment/luceneFilter/00901/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReaderImpl.java)
  public DocsAndPositionsEnum docsAndPositions(FieldInfo field, TermState _termState, Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException {
    PulsingTermState termState = (PulsingTermState) _termState;
    if (termState.docFreq <= maxPulsingDocFreq) {
      if (reuse instanceof PulsingDocsAndPositionsEnum) {
        return ((PulsingDocsAndPositionsEnum) reuse).reset(skipDocs, termState);
      } else {
        PulsingDocsAndPositionsEnum postingsEnum = new PulsingDocsAndPositionsEnum();
        return postingsEnum.reset(skipDocs, termState);
      }
    } else {
      if (reuse instanceof PulsingDocsAndPositionsEnum) {
        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, skipDocs, null);
      } else {
        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, skipDocs, reuse);
      }
    }
  }

commonMethod: 
(startLine=309 endLine=311 srcPath=/home/sonia/NewExperiment/luceneFilter/00902/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReaderImpl.java)
    boolean canReuse(FieldInfo fieldInfo) {
      return storePayloads == fieldInfo.storePayloads;
    }


, Instance #
frags: 
(startLine=182 endLine=218 srcPath=/home/sonia/NewExperiment/luceneFilter/00914/lucene/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java)
  public void testMultipleMatchesPerDoc() throws Exception {
    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, "seventy"),
            new MaxPayloadFunction());
    TopDocs hits = searcher.search(query, null, 100);
    assertTrue("hits is null and it shouldn't be", hits != null);
    assertTrue("hits Size: " + hits.totalHits + " is not: " + 100, hits.totalHits == 100);

    //they should all have the exact same score, because they all contain seventy once, and we set
    //all the other similarity factors to be 1

    //System.out.println("Hash: " + seventyHash + " Twice Hash: " + 2*seventyHash);
    assertTrue(hits.getMaxScore() + " does not equal: " + 4.0, hits.getMaxScore() == 4.0);
    //there should be exactly 10 items that score a 4, all the rest should score a 2
    //The 10 items are: 70 + i*100 where i in [0-9]
    int numTens = 0;
    for (int i = 0; i < hits.scoreDocs.length; i++) {
      ScoreDoc doc = hits.scoreDocs[i];
      if (doc.doc % 10 == 0) {
        numTens++;
        assertTrue(doc.score + " does not equal: " + 4.0, doc.score == 4.0);
      } else {
        assertTrue(doc.score + " does not equal: " + 2, doc.score == 2);
      }
    }
    assertTrue(numTens + " does not equal: " + 10, numTens == 10);
    CheckHits.checkExplanations(query, "field", searcher, true);
    Spans spans = query.getSpans(searcher.getIndexReader());
    assertTrue("spans is null and it shouldn't be", spans != null);
    assertTrue("spans is not an instanceof " + TermSpans.class, spans instanceof TermSpans);
    //should be two matches per document
    int count = 0;
    //100 hits times 2 matches per hit, we should have 200 in count
    while (spans.next()) {
      count++;
    }
    assertTrue(count + " does not equal: " + 200, count == 200);
  }

(startLine=221 endLine=260 srcPath=/home/sonia/NewExperiment/luceneFilter/00914/lucene/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java)
  public void testIgnoreSpanScorer() throws Exception {
    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, "seventy"),
            new MaxPayloadFunction(), false);

    IndexSearcher theSearcher = new IndexSearcher(directory, true);
    theSearcher.setSimilarity(new FullSimilarity());
    TopDocs hits = searcher.search(query, null, 100);
    assertTrue("hits is null and it shouldn't be", hits != null);
    assertTrue("hits Size: " + hits.totalHits + " is not: " + 100, hits.totalHits == 100);

    //they should all have the exact same score, because they all contain seventy once, and we set
    //all the other similarity factors to be 1

    //System.out.println("Hash: " + seventyHash + " Twice Hash: " + 2*seventyHash);
    assertTrue(hits.getMaxScore() + " does not equal: " + 4.0, hits.getMaxScore() == 4.0);
    //there should be exactly 10 items that score a 4, all the rest should score a 2
    //The 10 items are: 70 + i*100 where i in [0-9]
    int numTens = 0;
    for (int i = 0; i < hits.scoreDocs.length; i++) {
      ScoreDoc doc = hits.scoreDocs[i];
      if (doc.doc % 10 == 0) {
        numTens++;
        assertTrue(doc.score + " does not equal: " + 4.0, doc.score == 4.0);
      } else {
        assertTrue(doc.score + " does not equal: " + 2, doc.score == 2);
      }
    }
    assertTrue(numTens + " does not equal: " + 10, numTens == 10);
    CheckHits.checkExplanations(query, "field", searcher, true);
    Spans spans = query.getSpans(searcher.getIndexReader());
    assertTrue("spans is null and it shouldn't be", spans != null);
    assertTrue("spans is not an instanceof " + TermSpans.class, spans instanceof TermSpans);
    //should be two matches per document
    int count = 0;
    //100 hits times 2 matches per hit, we should have 200 in count
    while (spans.next()) {
      count++;
    }
    theSearcher.close();
  }

commonMethod: 
(startLine=49 endLine=55 srcPath=/home/sonia/NewExperiment/luceneFilter/00915/lucene/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java)
  public static Spans wrap(ReaderContext topLevelReaderContext, SpanQuery query) throws IOException {
    AtomicReaderContext[] leaves = ReaderUtil.leaves(topLevelReaderContext);
    if(leaves.length == 1) {
      return query.getSpans(leaves[0]);
    }
    return new MultiSpansWrapper(leaves, query);
  }


, Instance #
frags: 
(startLine=47 endLine=56 srcPath=/home/sonia/NewExperiment/luceneFilter/00954/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java)
  private void createBZ2LineFile(File file) throws Exception {
    OutputStream out = new FileOutputStream(file);
    out = csFactory.createCompressorOutputStream("bzip2", out);
    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out, "utf-8"));
    StringBuilder doc = new StringBuilder();
    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append("body");
    writer.write(doc.toString());
    writer.newLine();
    writer.close();
  }

(startLine=58 endLine=66 srcPath=/home/sonia/NewExperiment/luceneFilter/00954/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java)
  private void createRegularLineFile(File file) throws Exception {
    OutputStream out = new FileOutputStream(file);
    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out, "utf-8"));
    StringBuilder doc = new StringBuilder();
    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append("body");
    writer.write(doc.toString());
    writer.newLine();
    writer.close();
  }

commonMethod: 
(startLine=58 endLine=86 srcPath=/home/sonia/NewExperiment/luceneFilter/00955/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java)
  private void writeDocsToFile(BufferedWriter writer, boolean addHeader, Properties otherFields) throws IOException {
    if (addHeader) {
      writer.write(WriteLineDocTask.FIELDS_HEADER_INDICATOR);
      writer.write(WriteLineDocTask.SEP);
      writer.write(DocMaker.TITLE_FIELD);
      writer.write(WriteLineDocTask.SEP);
      writer.write(DocMaker.DATE_FIELD);
      writer.write(WriteLineDocTask.SEP);
      writer.write(DocMaker.BODY_FIELD);
      if (otherFields!=null) {
        // additional field names in the header 
        for (Object fn : otherFields.keySet()) {
          writer.write(WriteLineDocTask.SEP);
          writer.write(fn.toString());
        }
      }
      writer.newLine();
    }
    StringBuilder doc = new StringBuilder();
    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append(DocMaker.BODY_FIELD);
    if (otherFields!=null) {
      // additional field values in the doc line 
      for (Object fv : otherFields.values()) {
        doc.append(WriteLineDocTask.SEP).append(fv.toString());
      }
    }
    writer.write(doc.toString());
    writer.newLine();
  }


, Instance #
frags: 
(startLine=47 endLine=61 srcPath=/home/sonia/NewExperiment/luceneFilter/00974/solr/src/test/org/apache/solr/spelling/suggest/PersistenceTest.java)
  public void testTSTPersistence() throws Exception {
    TSTLookup lookup = new TSTLookup();
    for (String k : keys) {
      lookup.add(k, new Float(k.length()));
    }
    File storeDir = new File(TEST_HOME());
    lookup.store(storeDir);
    lookup = new TSTLookup();
    lookup.load(storeDir);
    for (String k : keys) {
      Float val = (Float)lookup.get(k);
      assertNotNull(k, val);
      assertEquals(k, k.length(), val.intValue());
    }
  }

(startLine=64 endLine=78 srcPath=/home/sonia/NewExperiment/luceneFilter/00974/solr/src/test/org/apache/solr/spelling/suggest/PersistenceTest.java)
  public void testJaspellPersistence() throws Exception {
    JaspellLookup lookup = new JaspellLookup();
    for (String k : keys) {
      lookup.add(k, new Float(k.length()));
    }
    File storeDir = new File(TEST_HOME());
    lookup.store(storeDir);
    lookup = new JaspellLookup();
    lookup.load(storeDir);
    for (String k : keys) {
      Float val = (Float)lookup.get(k);
      assertNotNull(k, val);
      assertEquals(k, k.length(), val.intValue());
    }
  }

commonMethod: 
(startLine=60 endLine=91 srcPath=/home/sonia/NewExperiment/luceneFilter/00975/solr/src/test/org/apache/solr/spelling/suggest/PersistenceTest.java)
  private void runTest(Class<? extends Lookup> lookupClass,
      boolean supportsExactWeights) throws Exception {

    // Add all input keys.
    Lookup lookup = lookupClass.newInstance();
    TermFreq[] keys = new TermFreq[this.keys.length];
    for (int i = 0; i < keys.length; i++)
      keys[i] = new TermFreq(this.keys[i], (float) i);
    lookup.build(new TermFreqArrayIterator(keys));

    // Store the suggester.
    File storeDir = new File(TEST_HOME());
    lookup.store(storeDir);

    // Re-read it from disk.
    lookup = lookupClass.newInstance();
    lookup.load(storeDir);

    // Assert validity.
    float previous = Float.NEGATIVE_INFINITY;
    for (TermFreq k : keys) {
      Float val = (Float) lookup.get(k.term);
      assertNotNull(k.term, val);

      if (supportsExactWeights) { 
        assertEquals(k.term, Float.valueOf(k.v), val);
      } else {
        assertTrue(val + ">=" + previous, val >= previous);
        previous = val.floatValue();
      }
    }
  }


]